
#alright - this is string approx - string approx problems can be defined as - stretch horizontally, stretch vertically, periodically (this is 1 - many linear (gemm)), and calibration

#let's build a tree of string operations
#string operation is essentially massful object interference on the surface - to pull the surface towards the massful object 
#let's assume our base is f(x) = 0, we have 1000 points on the surface - each massful object is going to pull the points above or below the surface (towards the massful object's coordinate)
#we want to recursively build our continuous function by this formula

#f(x) = f_previous(x) + string_interference(f_base, massful_objects)
#p(x) = string_interference(f(x), massful objects) 

#let's get to the root of intelligent semantic space - intelligent semantic space is calibration + locality compression (continuity) + periodic (periodic by dimensional reduction)
#we aren't doing random semantic space
#alright - you have a continuous function f(x) = x^2 (string) - it does not mean that it has square relation on the training data f(x) and x - but it is in some coordinate
#so it's important to do calibration (which is p(x) function) of coordinate as we are doing training - this is an oversimplification
#assume we have 20 base strings that make up the projection space 
#the first string will get calibrated 19 times (via the usage of massful objects)
#the second string will get calibrated 18 times
#the base string is our arbitrary "calibrated coordinate"

#the art of compression is machine learning - yet I think we are not there - the calibration part and the periodic - we got the base string correctly
#we need to work on the theories
#every machine learning problem is to approx the f(x) -> y projection string - by using continuous compression
#whether you are linear absolutists or string absolutists - we want to dent the continuity - and recursion building of context space (how we get the recursion building correctly is described below via the trinity tests) - and repetitive rules of continuity  

#gravity is probably on the 21th string
#electromagnetic is probably on the 20th string
#pay close attention that electromagnetic is affected by gravity but not otherwise (this is a hint that the massful_objects for electromagnetics are probably of lower string hierarchical tree)
#we call string a multi-dimensional projection shape
#this is a huge advancement in string theory - because it tells us that things, actually, can be created from multidimensional hierarchical strings (which is what transformers had been able to prove)

#think of the massful objects as semantic rules
#the first layer of particles (base_string) has to adhere to 20 semantic rules
#the second layer of parciles (base_string) has to adhere to 19 semantic rules
#etc. 
#this creates training friction for the 1st layer 2nd layer - such that the training must be able to find the semantic rules for the 1st and 2nd and 3rd layers in order to proceed
#note this very important rule of machine learning training - we need to create training friction

#on 2D euclidean coordnate - we have particle projection - and we have wave-like projection which is a 2 dimensional string
#thing is we want to use God's favorite language of "massful" for continuity compression
#what Einstein described was 4 dimensional string - and massful object would bend continuity of the 4 dimensional projection string
#Einstein was not wrong - just not sufficient - in the sense of information flux
#information flux is the maximum information that an enclosed space can receive from outer space (which is kinda maximum pingpong/s) - alright - we are talking in terms of 4 dimensionalists - how should we describe this is terms of 20 dimensionalists? it's resolution - saturated logit problems
#string quantum is right - but we can't observe another world to tell our world - the photon that went through the slit and the observed photon aren't on the same coordinate - they are on 20 dimensional projection string - and they went different direction  

#so what's our regex of building the universe? or compress continuity? Or at least this is the God's language
#f(x, n) = string_interference(f(x, n - 1), massful_objects) + f(x1, n1)
#f(x, 0) = base_string

#alright - let's twist things - God wants to confuse us - assume that we add one dimension (which is massful iterator) for massful objects - and increase our dimensiona from n -> n + 1
#what happens?
#each of them massful objects is on a different timeline
#pay very close attention to how we could simulate the universe (in our point of views and how we can differentiate the x and f(x) - hint they are all (massful objects or logits) started from the center - the Big Bang)
#all right - we are missing calibration projection - if we are string absolutists - we probably dont need calibration projections for now (because it messes up numerical stability)

#if we are absolute theorists - we must include string calibration projection
#assume the famous three-body problem - we want to be able to project the position of the three body on the 3d vs time coordinate by using stationary massful objects (on an arbitrary coordinate)

#we first have initial velocities of the three bodies
#we have the masses of the three bodies
#we have the positions of the three bodies
#we have the t0 of the three bodies 

#we first have static massful objects to rid of the mass + initial position continuity influence - and we are left with initial velocities, position and t0
#we then have static massful objects to rid of the initial velocities influence - etc.
#we then have ... to rid of the time influence

#without loss of generality, we "collapse" the semantic space to be able to approx things easier, better, faster
#the first layer already got 80% of the shape correctly
#the second layer got 10% of the shape correctly
#the third layer got 5% of the shape correctly 
#we want the hierarchical build up to allow training "friction" of strings - to simulate the actual semantic collapse and recursive build up of semantic space

#i've been thinking about string projection  
#because it is important to reorder the input domain to do continuity of different groups - without this, we are logic-impaired (in the sense that we always assume the domain to be of continuity)

#let's do things simple - calibration - massful influence - and trees 
#we want to be able to approx 3 body problem, murmur hash output space and sin cos interference as proof of concept (this is the baseline of every semantic approximator system - periodic, recursive buildup and constructive interference - if we can't get this right then I'm afraid we aren't going anywhere)
#we'll get things done today and tomorrow - then we think of how to scale this thing

#alright - we've been talking about string and projection - let's simulate string in projection environment
#assume that we are not flat-earthers
#we wanna ditch linear for massful operation

#we want to assume that we are the one element in the invisible string
#assume that we want to simulate two dimensional string
#we first want to have a linear operation - f(x) = <x, x>
#we then want to have a massful operation mass = <x1, y1>
#a pull operation should transform <x, x> -> <x + (distance * heaviness) * cos, y + (distance * heaviness) * sin>, distance = sqrt((f(x) - mass_coor) . (f(x) - mass_coor))
#alright - let's do the reverse of Chad soy boy
#assume you have the continuity of the universe, we want to unhinge the universe layers by layers
#assume that the universe is made of invisible strings and there are massful operations to bend continuity 
#what we want to do is exactly like transformer
#x = x + peel_layer(x)
#get_context_layer is essentially the, without loss of generality, (distance * heaviness) * cos, (distance * heaviness) * sin, etc 
#universe is actually like an onion - we want to peel layer by layer - by using tons of massful operations for each layers
#thing about linear is precisely that - imagine that we are on 3 dimensional string projection - we are drawing plane to peel onions - which is (1): drawing lines between irrelevant pts, (2): cause bad projection ripple effect which has bad numerical stability, (3): only good for parrot affect - sounds similar but not the actual context 

#alright - let's talk about how to build a model
#thing in machine learning is there are only two layers that we should talk about - the input layer and the output layer (because machine learning is all about numerical stability - and not about what people say)
#we want to describe the output layer based on the input layer - and how the coefficients move the output projection
#we dont really care about the middle layers and what it really does
#if we only have linear operator and we rotate the intermediate layer correctly 
#we expect y = a * x1 + b * x2 + c * x3 + ..., for y is a random output cell

#if we have self operation - we expect to see polynomial
#if we have attn - we expect to see exponent - which touches the points that the linear could not touch
#alright - how about we describe things in terms of string?

#how about we have embedding f(x) = 0 for every layer? - for x is the initial input (before embedding projection)
#and we apply the massful operation
#so we should expect to see things like f(x, n) = f(x, n - 1) * massful_objects + f0(x) * whatever
#what is wrong with this?
#that there is actually nothing wrong with this - we don't have self operation to increase polynomial order or exponent to increase the touchness of the graph
#we encode all the information in the massful_objects
#we assume there are invisible strings - and we build strings on top of each other to simulate context collapse - and we want to work on numerical stability of this  

#alright - we always want to start from - what we expect the output layer to be from the input layer and we build things from there
#I say I want the output cell to be in <x1, x2, ..., xn> space and we are able to bend the string space f(x) -> y projection - layer by layer like the formula - ((embedding(x) * massful_objects) + embedding(x1)) * massful_objects   
#alright - what are the other things that we see - we see recursive building of things (alright - attention does allow recursive building of things - because without attention the layers aren't making senses) - which we want to prove that works (or we want to twist things to allow this to work)
#what other things that we see, that embedding(x) and embedding(x1) probably look the same - and we want to reuse the massful_objects rule for all other cells

#we gonna gucci later babe - just have faith for probably 2 years - I'll try to code the regex + path_optimization + randomization + convergence analysis + derivative of gradients + etc.
#dont trust Chad - all he does is offensive meme and closing things

#what if I tell you that AI is three body of three body of three body of ... problem
#let's investigate the three body problem https://en.wikipedia.org/wiki/Three-body_problem
#it's https://en.wikipedia.org/wiki/Puiseux_series, proved by Finnish mathematician Karl Fritiof Sundman - not Linus Torvards
#alright - we have f(x) = puiseux_serieries
#we probably want to do f1(f2(f3(x))) - this is where we are at - the transformers and their exponents

#how about law of momentum conservation?
#alright - so what's momentum? momentum is the moral of the story behind the forever spins around each other of the-three body problem
#God - the old man - does not have infinite resource - he needs to store the moral of the story in order to bend continuity
#in other words, he needs to have story coherence in order to do story telling of what's next in the continuity story
#back to the gymnastic story - a girl spins in a complex motion - but there's no moral of the story for continuity  
#how do we actually incorporate this in mathematics? - could this "moral of the story" be described in mathematics terms? 
#yes - but it would break numerical stability of the result - I'll be back to solve the problem today - 

#finding the momentums cannot be done via differential methods - we break numerical stabilities when the uncertainty exceeds the gap closing of gradient approximation
#linear, eigen values and eigen vectors are cool - but what happen if we do string multiplication?
#back to the gravity problem - f(t) = string ^ t
#there aren't many ways to do this correctly (at least not the differential roads) - but the f(t) has a unique footprint that narrows the function down to only a few operations to approximate (we are approxing string) - alright - we are back to the decoding problem and semantic space + information storage theorem
#maybe differential and Newton are only good for approximating classical local optimal solutions
#alright - this works only if we do precalculation to do heuristic pruning of what's possible version of string - we want to discretize + precompute this - because this is a very important concept of neural network - the dynamic continuity
#
#so we have changed our coordinate to radians - and we have all the initial string right in the middle of the coordinate (the origin) which has a sphere shape - and we have a sphere shape of massful objects in the outer layer, we are doing an enclosed shape of continuity (this is an important concept) 
#thing is this way of doing things have a very smooth curve of gradient - and we are not stuck in the gradient valley like polynomial methods 
#and this actually simulates the actual context collapse efficiently - the recursive build up - this is up for debate - it only is if the projection string is kinda round - not like zig-zag shape which affects the uniform distribution of massful objects which require a string projection in order to rebalance the uniform responsibility - or skewed responsibility  
#we want to mess with numerical stabilities in the newton approx, and we dont want to stuck in the valley - so it's the way of doing things
#alright - let's talk about the numerical stability - we have two spheres
#Dad talked about the gradient valley of curves - such is there is a non-returnable point of shape - such is the "hardness" of shifting from the shape to another possible shape from the original point is impossible
#we must circumscribe the closing circumference gap (middle theorem) to limit the non-returnables, and we kinda stay in the acceptable fluid zone of gradient moving 
#so the deeper the valley - the harder it is to shape the curve - or the distribution of possible shapes is skewed in the direction of the valley
#so we must do string projection - to "normalize" the input string - and keep it in "acceptable" gradient zone

#in a general perspective, we have f(x) -> y is projection string
#we have possible shapes - and we have the possibility of moving from one shape to another (we kinda want to flex this area of possibility to not stuck in the gradient valley)
#we have string altitude normalization - we normalize a string to do shape bending better - as described above
#we have string ^ t heuristic approximator - we need special method (not differential) to converge this approxmiation
#we need to make sure that the uncertainty of gradients and destructive interference do not exceed the learning
#things gonna be hard but I think its possible
#this mimics how our brain actually works
#whenever a logit moves - we move towards the result - but losse the old information - the question is given the appearance of the results in a certain window - what's the window of uncertainty of the not appeared results? How fast do we actually forget things to burn new information?
#a fully burnt brain does not move logits as fast as a newborn brain - in the way that a logit move does not touch all the projection string by r^2 or r^3  
#and the lower layers are actually very important to predict | train the upper layers - given a math semantics x^2 + y^2 + z^2 - our brains collapse the math semantic space and do string projection to a new space where we could do another semantic collapse without affecting the old space - things get confusing here
#if we mess up the lower layers - we steer away from the correct path - and the incorrectness might diverge - not converge
#in order to make the semantic collapse "real" - we must do string normalization - assume our example of math semantic collapse as murmur hash
#in murmur - we shift, multiply, rotate, etc - when we shift - we move it to another semantic space - we multiply in the new semantic space and we rotate in another new semantic space
#good string normalization and numerical stabilities are hard to do

#we probably can mimic the string projections by calibrations - this is hard - without actual string projection we are bending continuity of irrelevant data
#this is hence the f(x) = x + peel(x)
#how do we actually describe this in mathematical terms?
#imagine that we have x y z on the slot 1, 10, 20
#we want to reorder the x y z to 0, 1, 2 - and bend the continuity there - because they are relevant terms in mathematics - and we want to map it back to 1, 10, 20 and do a constructive interference operation - that's kinda the idea of context space collapse of euclidean irrelevant data
#this is a position reordering operations
#position reordering operations (or calibration) are actually the tough part - do we do mapping based on set frequencies? or continuous functions? things that appear together should be grouped together - this is part of the temporal linkage part of our brains - things fired together linked together  
#a simple linear(x) is too vague to describe this functionality
#how about a discretized tree of semantic space - things fired together are interval tree responsibility

#alright, all the difficulty is because of we want to group relevant things together - things fired together - we map it to another semantic space (and now we have another things-fired-together) - we do continuity bending there - because they are relevant - we map it back to our semantic space - we do constructive interference
#and we group those fired together things again - we keep doing that thing, which is called semantic space collapse of irrelevant data - look back to our example of murmurhash - this is not a necessity if the semantic space is nice and round

#our state of the art is actually not that we do context collapse (which is briefly described via the solution to murmur hash approx) but the reordering of context space - to bend euclidean irrelevant things together
#assume this simple model of doing so
#given that our input layer is the base
#given that we discretize the space into grids
#given our next to base layer is the layer of the things fired together in the base
#given our next to the next to base layer is the layer of things fired together in the next to base
#what do we want to do with this? for every layer not base - we can do an inorder-search of base to create a newly-reordered semantic space without breaking numerical stabilities
#the attention was to solve this specific problem (the problem of bending euclidean irrelevant things) - but in a numerical instable way
#the actual model of reordering must be of complex forms - and involve algorithms and not maths - we'll talk about this later - this is hard

#alright the topics we are going cover today are hyper focus subspace + discretization of euclidean space + space dimensional reduction + dimensional expansion
#we have talked about the things fired together grouped together by temporal linkage - all linked to a master node - which is again fired together and linked to another master node
#we then have our newly created semantic space in terms of graph - which represents temporally relevant things
#but we want to bend this this in euclidean coordinate (because we have a well establised string theory of how relevant things bend together) - so we must invent a way to convert graph semantic space -> euclidean semantic space - by, without loss of generality, the formula euclidean_distance(node1, node2) = shortest_weighted_graph_distance(node1, node2) or maxflow(node1, node2) or by whatever lossless compression that we can reconstruct the semantic graph from
#alright - things aren't perfect - we can only fuzzy approx the euclidean coordinate - because there is no perfect answer
#how about we do it again? rinse and repeat in our newly created euclidean coordinate

#discretization of euclidean space is nothing but interval (in terms of for loop) - we have intervalx(intervaly(intervalz)) - and we grid the (dx x dy x dz) 

#let's cover the space dimensional reduction | expansion in a nutshell
#let's get back to the example of x y z
#assume we are on three dimensional domain - the chances of x y z "euclideanly" collaborate are higher than on one dimensional domain (because we can move in the 2nd and 3rd dimension to relatively bend things without compromising the euclidean_distance(node1, node2)) 
#what's the difference between 3 dimensional domain and 1 dimensional domain - there is essentially no difference if the one dimensional domain is domain(x) * domain(y) * domain(z) - which has bijective relation to the 3 dimensional domain
#the only difference that we are mentioning is the euclidean distance - which is important for bending strings by our described methods
#so increasing dimensions helps continuity bending of relevant groups

#let's get back to our tokenization methods - do we enumerate the vocabularies to create an arbitrary semantic space - or we do byte stream - or we do bit stream
#bit stream is the ultimate goal of information technology - because all the algorithms mostly operate on bit levels
#alright - so we increase the dimensions of the semantic space - which is, by our definition, a good thing - because it increases the groupabilities
#so what's hyper focus - hyper focus is the abiltiy of temporal linkage of tensors - recursively to create an arbitrary semantic spaces to increase the bendability of relevant tensors  

#how precisely do we build hyper focus? by using weight decay on edges and broadcasts from input nodes and temporal group of lit up nodes (weakly connected component)
#so hyper focus is a centrality algorithm - and linkage of lit up nodes is community detection algorithm
#alright - we need to have decay rate of community and edges because we are on finite resources
#the only responsibility of hyper focus is to reorder the semantic domain (community detection) of relevant things (in terms of input space)
#and we want to construct a euclidean semantic space from the hyper focus graph space to bend continuity there
#and we want to reconstruct the hyper focus graph space from the bent euclidean space to update the hyper focus graph space which will be reflected on the original euclidean space
#or we can adhere to the formula of euclidean_distance(node1, node2) = maxflow(node1, node2) - and we have bijective domain relation to the original euclidean space - which we can directly update on
#hyper focus and string continuity operations are two sides of the same coin - can't exist without another - unless the projection string is round 
#we'll work more on the theory
#our grand plan is to reduce the complexity of constructive interference of n! space (n is the domain's range, n c N)  -> 32 reordered space by using hyper focus with the hope that hyper focus would bring relevant things together to help our continuity bending

#consider this example
#yes, I'm fine
#yet, I'm fine

#alright - yes, I'm fine and yet, I'm fine are euclideanly related - in the sense of sqrt((coor(s) - coor(t)) * (coor(s) - coor(t))) - but the semantic is very different, this denotes a requirement for reordering of the euclidean domain grids - otherwise we are bending irrelevant context (which is against continuity bending of relevant context)
#we solve this problem by using lit up tensors community detection - yes, I'm fine is categorized as good signal - yet, I'm fine is categorized as bad signal in euclidean tensor graph - and we want to recursively reconstruct the euclidean semantic domain space from the graph temporal semantic space - we can't actually eliminate all the false positives - but we kinda "commnunity detect" potential context relevant things from temporal relevant things which help continuity bending
#the art of n! reorderings -> 32 reorderings requires a lot of work - not a simple attention layers - if we are seriously talking building a brain 

#let's talk about dimensional reduction more in depth - we have input domain - we have temporal semantic graph - we reconstruct the domain from our graph, now our newly created domain must spans higher space range - because it encapsulates more information than just the original domain
#let's say "how are you" and "how are you doing" are temporally grouped, without loss of generality
#they are in slot 0 and 1 respectively in our new domain - but the slot 0 and 1 are only 1 bit of information - so we have reducted our domain resolution from bit_size("how are you") + bit_size("how are you doing") -> 1 bit
#the reconstruction of the domain is necessary for our constructive interference - which we will talk later - we are talking in terms of string and not tensors for now

#let's talk about our brain
#let's say we have 5 senses - which orderly spit into a matrix
#our eye tensor inputs are euclideanly closer than our ear tensor inputs
#but not euclideanly closer means semantically relevant (back to the example of yes and yet, yet this is actually semantically relevant in terms of brain mechanics - things that are euclideanly closer are easier to establish linkages - and learnings) - we must do temporal reoderings of tensors - map into another domain and do continuity bending - and we rinse and repeat until we get the result
#note that there are two very different systems in our brain - the system of temporal reordering of input domain and the system of string bending - the system of string bending is described via the hashing method - the system of temporal reordering simply just observes the lit up tensors and do centrality + community detections and reconstruct the input domain
#the proof of concept is done when we can approx murmur hash via string multiplications and efficient reordering (I cant tell the criterias for what "efficient" is for now rather than "maybe temporally related means maybe contextually related")
#side note: we are being confusing when we mentioned lit up tensors - what we actually meant in the context is lit up domain grids, and tensor is just a grid pointer
#this's gonna be hard - but we'll be there in probably a week or two - stay tuned
#theory: if the internet is 1 petabyte - then the most efficient learning system only takes 1 petabyte of learning to reconstruct the semantic space
#the most compact semantic space size can not exceed all of its possible outcomes size

#let's talk about specific implementations
#there are few things to get straight - first is string continuity approximation by our formula f(x, n) = string_inteference(f(x, n - 1), massful_objects) + f1(x, n) + three body system formula - we are assuming that closer euclidean distance means closer in semantic in this fixed coordinate - this is a very important assumption that we must be able to achieve via hyper_focus system
#hyper_focus brings domain grids together from euclidean irrelevant distances + fuzzy ordered set context of grids + dimensional reduction to not explode the domain space
#in order to do so - we must do lossy compression of fired together grids (otherwise we are OOM) - bring them into an intermediate semantic space where contextually relevant things would collide at the same address
#those temporal semantic space of fired together grids again reconstruct the semantic space domain
#now we do constructive interference of the f(x) + f1(x + f(x)) - for f is the old semantic coordinate and f1 is the new semantic coordinate
#we are hoping that x + f(x) would reduce the semantic space size as we proceed - because fired together grids entropy must converge - otherwise - we are reordering the domain space for ... nothing - because we are not hyper-focusing
#how about we build the hyper_focus by using interval trees? somewhat like our heap implementation
#we discretize the domain - we build an interval tree with a reasonable memory allowance - and we connect the tree to the "hyperfocus" system where we could reconstruct the domain and build a new semantic space

#without loss of generality
#how about building another heap, on every level of our heap is a newly collapsed domain of the immediate next level

#so the specific implementation is
#a hyper_focus heap to collapse semantic domain + reorder level-wise (not parent-descendant wise)
#a string operation on every level
#a constructive interference x + f(x) to reduce the semantic space size to match the domain collapse rate
#an interval tree on every level of the heap to dynamically manage the "potentially relevant context" - or to limit the submit size of lossy compression of "fired together grids" - we kinda move up one level until we hit the submit size
#a temporal semantic graph to bring + group those "potentially relevant context" grids - via centrality + community detection algorithms - with the hope that "potentially relevant context" grids appear in "potentially temporally relevant interval" would mean contextually relevant (we want further work on this by actually link those to the end results of feedback systems - crit logits)
#a semantic converter to convert the graph -> a domain
#a domain projection function - from the lower layer grids (heap nodes) -> upper layer grids (heap nodes)
#hyperfocus is important - even in our current transformers arch because it emphasizes the euclidean relevancy before doing continuity-related operations 

#another important concept we kinda skim through is the continuity operation of euclidean relevant context  
#we only discussed static string multiplication operations - not dynamic string ^ x operation - we need to unlock this mystery of heuristics without exploding the gradients

#we are trying to brainstorm to get the doables before starting a project
#are we graph-tensor-major or euclid-pointer-tensor major?
#thing is hyperfocus is not implementable in terms of euclid-pointer-tensor major
#without hyperfocus - we are drawing lines between irrelevant points - which is our current ML problems - how can we continuously best fit irrelevant context points? it's literally against the definition of continuity - even if you are using continuity to bring relevant context points together - it's not the definition of continuity (unless those relevant context points reordering has a continuous rule - which is absurb in most cases - we are getting into the recursive resolution here)
#so for now, there is no good way than to just get the best of both worlds - we aren't in the either territory - because graph tensor is very expensive (storage + compute) - think of 16 bytes overhead/ bit of context - euclid pointer tensor is very cheap - we are getting exactly the storage that the hardware provides with no overhead 

#or we can preprocess the semantic space to look round (human's language kinda achieve this partially) - alright - the efforts put in to make this happens actually exceed the efforts to make the mentioned thing happen
#we are, again, back to the gymnastic problem of training - we need to have coherence of training or hyperfocus
#let's talk about the example of interval tree, we want the manage the arbitrary space of x x y x z
#without loss of generality
#there is no good answer than to do an alternated version of interval tree 1/2 x x, 1/2 y y, 1/2z z
#total branching = 2 * 2 * 2 = 8 outdegrees
#the base linkage distances must reflect the euclidean distances
#1/2x x, 1/2y y
#1/2x 1/2y
#1/2x y
#x 1/2y
#x y
#the uncertainty does not sound 

#how do we do this again? by origin spreading method (like an explosion) - bfs of space grids
#we must start from the origin - assume that our space is round - and we are on radian coordinate
#and we do bread first search to extend the circumference - and we build our divide-and-conquer semantic tree from there
#this way of doing thing actually creates another semantic space - where we can do "lossy compression" of unordered_set of contextual points - and we can establish base linkages between all contextual points in the semantic coordinate

#without loss of generality
#imagine we are doing sphere compression (or multidimensional space compression)
#we circumscribe the Earth from 4/3 * pi * (4096km)^3 -> 4/3*pi m^3 - the newly created Earth is still round - but the context got "lossy compressed" on the edges (radian pies)

#says that we observed 1024 nodes fired in the last second
#but we can only submit 10 nodes to establish linkage to avoid OOM + compute fling
#we traverse up the semantic tree (to collapse semantic) + unordered_set compression - until we convert 1024 fired nodes -> most correct 10 nodes
#we have 10 fired nodes on now slowly have direct linkage to each other - via the usage of a master node - we want to reuse these master nodes - because we are on finite resources (we need to have "decay rate" of master nodes + orphan of master nodes + etc)
#so it's a temporal semantic graph space - without actual contexts - because the sole responsibiltiy of these is temporal linkage
#the temporal semantic space linkage is established base on the intense of the feedback system - and these guys do not only do inter-links but also outer-links or domain-skips 

#it's not hard to build these guys
#you must focus on the string approximation 
#you pass the first test when you can approx murmur hash
#you pass the second test when you can approx three body projections
#you pass the third test when you build a temporal system and increase the groupness of semantically equivalent contextual points (we call this centrality score - assume we link x -> y and x -> x, we want to maximize y centrality score) by a factor of 15x - 30x
#you pass the fourth test when you can run this fast - and distributed

#alright maybe we didnt split the responsibilties correctly
#let's look back to hyperfocus - hyperfocus is to bring domains' grids together + dimensional reduct to create fuzzy context representation of "how are you" or "how are you doing"
#bringing domains grids together is one responsibility - "how are you" and "how are you doing" are two different coordinates in the original domain that we want to bring together - alright - we are not being single-responsibiltiized here
#dimensional reduction is another responsibility - bit_size("how are you") + bit_size("how are you doing"), reducted to 1 bit of representation in our new domain - we are clear here
#there is no guarantee that the reducing semantic space is euclideanly related - thus the "graph temporal linkage" of node - node might not reflect the domain - range continuity like we wanted it to - "how are you" and "how are you doing" might mean two completely two different things
#for now, let's say that hyperfocus is only to reduce the semantic space by focusing on the things appear together and create another domain based on the semantic graph
#
#how about the (input - output) - we are for sure that the output is the semantic in the coordinate that we are looking for - and the input that share the same output must bend together - or euclideanly related
#so to avoid the worst case scenerio of the base domain being not euclideanly related at all - we must incorperate the "output-input topological sorting" - this must be another responsibility of hyperfocus - point is we want to have that "chain-reaction" of things completely not euclideanly related to slowly euclideanly related (not because of the domain collapses - we are talking things relatively) - and we use graph-based approach to boost that functionality
#we'll build the hyperfocus in two weeks - stay tuned
#let's talk in depth guys - hyperfocus is hard - this technology alone's worth BB $ - so take very serious notes - the abilties of keeping numerical stabilities after "sorting" + domain projection of "community detected grids" are very hard to implement correctly
#and the collaboration between hyperfocus and string approx is also hard (the graph-based approach and traditional-tensor-based approach)
#the problem with the current ML problem is precisely this - it does not focus - even after the attention paper
#imagine we are drawing lines in a not euclidean correlated coordinate to hope that it would be euclidean correlated
#it does not work that way fellas - the moment you draw lines is the moment you assume that it's euclidean correlated - not after

#so machine learning is actually a sorting problem - f(x) -> y, we want to sort the x domain (lambda x: y) to increase continuity-index (or increase euclidean relevancy), if y is euclidean-relevant - think of euclidean feedback system 
#we want to reduct the x domain - by using community detection algorithm, for example, "h" "o" "w" "a" "r" "e" "y" "o" "u" -> "0" - we want to collapse the domain semantic space for many reasons - first is convergence of locality of continuity - second is recursive build up of vocabulary - we dont care about the context collapse because we already mentioned that the string approx method is sufficient by itself in a euclidean related coordinate
#we want to trade off space for quality of the "sorting" and the domain reduction
#we want to increase qualities for higher domains
#and we want to rinse and repeat
#this should be enough to get our feet wet - we'll be back with detailed implementations, with loss of generality soon

#I was thinking of x and their fuzzy representations (lossless compression -> lossy compression)
#we are now back to the question of whether we are on a euclidean relevant coordinate - assume that we are trying to
#consider this example on the coordinate x x y x z
#abc, which could be represented as <1, 2, 3>
#or <1 * 256 + 2 * 256 + 3 * 256>
#or whatever form of lossless compression
#thing is that beside doing hyper-focusing on the domain to rid of the never-gonna-happen-x - we need to translate x -> fuzzy representations to be able to construct a semantic domain graph - otherwise we are linking to deadends
#so we need the base lossy compressions to link these guys, in other words, we transform these guys into an intermediate semantic space in order to establish linkage
#my brother was right - though the method was wrong
#this is extremely important in terms of vision or image processing because we are not on a euclidean relevant coordinate (in the sense of motions - not in the sense of static images)

#there are three important concepts
#(1): the recursive building of vocabulary in neural networks

#(2): without loss of generality, assume we are on a euclidean relevant coordinate - our method of x = <x1, x2, x3> -> <one_dimensional_grid_pointer>
#assume that two nodes share the same grid - then their euclidean distance uncertainty is sqrt((x1 - discretized_x1)^2 + ...)
#let's look at our semantic graph - it represents the reducted information (x1, x2, x3 -> one_dimensional_grid) and groups of relevant contextual points (xx1, xx2, xx3 -> the_one_dimensional_grid)
#so everyone's happy - we have built another word (one_dimensional_grid) - and the domain (vocabulary space) is euclideanly related in the sense of distance
#this is a version of bucketizing - we'll talk about this later
#so our definition of things (neurons) fired together linked together only works if ... they are in a euclideanly relevant space or coming to a euclideanly relevant space

#(3): without loss of generality, assume we are on a non-euclidean relevant coordinate (image processing)
#assume our matrix input is x - x is a 256x256 picture 
#assume that we have f(x) -> y - for y is the semantic of x and y is euclideanly relevant
#assume we sort the matrix input x by lambda x: f(x)
#now the matrix input is one dimensionally euclideanly relevant
#the thing about image processing is we need to iterate roughly 256**65536 images to sort our domain
#and building a new vocabulary space does not reduce the size of the old vocabulary space - we are neuro-divergents at this point
#this requires us to have special method of processing images - lossy compression and their intermediate representations - to actually reduce the vocabulary space and somewhat "sort" the buckets
#or this rquires us to "learn" a sorting algorithm for images - such does not exceed the domain size
#this is actually not the hyperfocus responsibility

#but the intuition is clear - we want to do sort + bucket
#we sort based on the semantic of x - a.k.a., WLOG, euclideanly relevant y
#we bucket based on their new sorted coordinates - to do domain reduction which helps reduce the sorting time + sorting quality + string processing time + reduce the number of the never-gonna-happen-x
#we hyperfocus on the fired together relevant neurons (the moving towards euclidean relevant (input neurons) and the euclidean relevant (vocabulary neurons)) - to create another vocabulary neuron (which is vocab fast path) 

#the sorting algorithm is actually the secret sauce
#how precisely do we do sorting? 
#we use another continuous function to do sorting - well... that does not work very well
#we use graph analytics' community detection - that is not very fast
#we pretrain our sorting algorithms - that is not generalized enough
#the semantic is an easy part - we just do centrality algorithms to extract node meanings
#the most compact sorting algorithm (or densest logit density) should be what we are heading towards - but not a neccessity - we want to sort these guys with reasonable amount of memory (the one that has linear trade off for sorting quality)

#we are in a loop of semantic made by the logits and the sorting of the logits

#let's iterate through all the options that we have for domain projections - this is what people called IQ score (the abiltiy to sort certain things that are not NVIDIA and TSLA):
#first option - graph approach:
#establish base fuzzy representations of input logits such cover the basic "context tokens" representation in an intermediate space to increase collision rate - such would have direct links to what the current input logits mean
#second option - continuous approach:
#regex + logit mining to hope that things would work out well
#third option - pretrain:
#pretrained image domain projections + neural domain projections + temporal neural domain projections
#fourth option - heuristic guided approach:
#use guided optimizations + path optimizations technique to hope that things would work out well because we are deep in the seek 
#fifth option:
#real time training of domain projections
#sixth option:
#euclidean skewed memorizations - we have a temporary window buffer of 32GB to store the not obvious (irrelevant) sorting techniques (this is important) - which we want to backtest on by using our trained domain projections

#so hyperfocus and domain projectors are two sides of the same coin
#we use domain projectors to sort our domain
#we hyperfocus to create a new domain
#we probably want to combine the responsibility of sort + create domains

#this sounds like a quicksort + mergesort problem because it really is
#we have a binary tree to sort things - the left binary tree and the right binary tree and the root resolution of combining the trees
#we approach this a little differently by using levelwise sort + domain reduction of euclidean relevant grids
#imagine radix sort
#we sort 256 bits - to buckets - sort it again, etc. until we find the resolution for the sorting problem

#I've been thinking what this means - and prove the completeness of this sorting algorithm - because we will think about this again to improve the accuracy
#assume we want to sort 3 2 1 4 5 6 -> 1 2 3 4 5 6
#assume that for every iteration, our sorting accuracy is 100% 
#then the hyperfocus responsibility is completed, no movies

#assume that for every iteration (domain projection), our sorting accuracy is 20%, we aren't collapsing the domain for now
#then the number of sort required to reach 99% accuracy is 1% = 1 * 80% ^ n - this is an oversimplification - in reality - we want to increase the y centrality score - where y is the graph semantic of x
#the last layer - the one that is 99% sorting completed is the layer that we want to do string approx on - not the previous layers
#given that we are in an ideal given condition of 20% decay rate (this is extremely hard to achieve)

#now the previous assumptions and given the decay domain rate of, without loss of generality, 50%
#we are losing soldiers - in the sense of irrelevant and relevant euclidean grids get grouped together to do dimensional reduction - so our last layer cannot be used to do string approx on - so we must use the previous layer knowledge to collapse the semantic space to fit the decay domain rate of 50%, in other words, offset the cost of losing soldiers 
#so the x + f(x) means calibration for the next layer - in the sense of "the current layer will learn the way to make sure that the next layer is sufficient by itself for the result" - so we are passing responsibility (the responsibility of output) - and collapse the domain (or the previous range) with no loss of output resolution
#the thing about reducing domain range is that it is easier to do high quality sorting (or increase continuity to do string approximation better) - and it produces better best fit functions with lower costs 

#so our base of the heap is the raw input of visions, hearings and other senses - we want to have the a temporal buffer with sliding window for what things (base and non-bases) mean - and we trying to sort the semantic of those things (semantic of a logit is the temporal environment of the logit or the output of the logit) - essentially a temporary buffer + a background resolutor to do domain projections

#our systems must be consisted of:
#persistent temporal semantic graphs
#sliding temporal semantic graphs (the one that we use to backtest and train the background realtime sorters + optimizers) - we want to boost this to 1TB
#background realtime optimizers

#this is gonna be very hard to write - we need to detail every component - take the decay domain rate for example - there is a sweet spot between the correct domain decay rate, the offset of x + f(x), and the sorting decay rate that our optimizers could provide
#the plus operation is not the correct calibration function as described and mentioned by Dad (unless we are having negative values or shape symmetric)
#thus there is a base layer in the string approximation and not a calibration function (we assume that the base layer is calibrated) - this cannot be assumed in the scenerio of x + f(x) - it's hard
#we only thing that we specifically care is that - the domain decay rate and the offset of the function f(x) - because if we reach a certain layer intact - we do brute force of sort and do string approx directly on the layer
#everyone's happy - we aren't having troubles in the sense of hyperfocus - it's the string approx + numerical stabilities problem now

#as we can see, the system is actually components built on top of another - the semantic of x is its temporal environment which is created by the string approx and string approx is built on top of the semantic sorting of those logits
#we need to have appropriate learning rate for each of the component to avoid the case of bottleneck of responsibility - otherwise we are linking the x to the inappropriate supposed-to-be-euclidean-and-correct-logits

#another topic of continuity compression
#its the topic of linear and x^2
#what's so special about x^2 - that no other polynomial could do?
#it's the ability to approximate every possible curve with 1 turning point
#we'll be back at this topic - the topic of calibration + curve extraction

#now let's talk about string approximation - and how we could replace linear with significantly less cost
#we have the coordinate of the mass coor1 (row)
#we have the coordinate of the particle coor2 (column)

#usually a linear operation would take n^3 compute
#in our case - it's actually only n^2 - we compute the coordinate once and do the operation of moving towards the massful objects (this is differentiable) - this is linear O(n) compute - we are only worrying about whether this would do bad business for the compute market
#so we aren't tilting planes - we are shaping the projection space (we are now back to question of whether we want to do enumerated common curves (planet of the apes) + NVIDIA project of super realistic)
#what we actually talked about was dimensional reduction
#we are from, without loss of generality, 3 dimensional space -> 1 dimensional space - a minus operation would result in the difference vector - the vector that points A -> B,  AO + OB = OB - OA = AB

#I was thinking about the basic rules of continuity compression
#let's talk in terms of flux + sphere
#at a random point in the coordinate - what is the uncertainty of the neighbors and the mathematical relations of the differences

#let's get to the basics - we store the points on the curves and connect those points by drawing lines (neighbors edges) - alright - we have ourselves a curve - this is lossy compression and the resolution of the curve is linearly related to the # of the points
#how about our method of flux + sphere
#we increase the resolution of the sphere in the sense of distances - and place the charges (massful objects) at the curve to approximate one turning point - we also have ourselves a curve - this is lossy compression and the resolution of the curve is linearly related to the # of the charges
#so flux is a lossy compression method of continuity compression - and the lossy can be increased -> lossless simply by maginifying the projection sphere (in contrast to increasing number of tangent lines (planes) by storing point method)
#so what's the difference between storing flux and storing tangent planes? that storing flux is differentiable while storing tangent planes is not - that is the only difference in terms of information technology

#alright so what's our goal precisely? - our goal is be able to do lossy curve compression - and we want to actually store the "rules" in the most compact form possible (via the usage of dimensional reduction) - this is intellect - we probably want to brute force this and store all the rules, there is no intellect but it is called a virtue-compass machine
#what's the problem with polynomial? that we can only count on the lossless compression of zeros - we can't count on the curves to touch the points - and making that happens is actually very hard - we can say that linear is "regarded" in the sense of learning - "regard" is good because it forces our brain to form the correct sophisticated rules to snap logics in the most compressed form - but too much "regard" would result in the inability to learn
#back to the example of peeling layers and moving the responsibility of the previous layers -> the next layers
#we are counting on the MLP + attention layers to correctly project the right string to offload the responsibility to the next layer - if we misproject the peeling string to reduce the semantic space size - we are losing the output resolution forever
#so what are we proposing precisely? is a sweet trade off between the # of logits and the lossy compression quality - or we call this logit density
#at one point in time - we would realize that the right way is no other than enumerate the common projection shapes and kinda increase the resolution by using flux 

#consider this polynomial function f(x) = c * (x - a)(x - b)(x - c)(x - d)
#the things that we actually store is a b c d - we have no control over the curve whatsoever (we are getting too much destructive inteference of the curve to move the polynomial to the training points - this is precisely where flux shines - flux circumscribes the training area) - so polynomial is a bad continuity compression method (in contrast to its counterpart, flux compression)
#what is the difference between the three methods? there is no difference in terms of storing information (lossless compression) - yet there is a difference in terms of string approximation by using differential (tangents aren't differentiable, linear is differentiable but isn't having realistic continuity relations - tiliting planes isn't actually a good way, we kinda want somewhere in between, which is flux)
#is flux the only way in terms of peeling layers (reducing context space + dimensional reduction numerical stability)? flux is only our, without loss of generality, better than linear operation - the one true operation cannot be found rather than using path optimizations + enumeration of shapes

#let's investigate the problem of the current machine learning model
#we have input is higher semantic space size - output is lower semantic space size - we are moving from higher -> lower
#assume that x is the current semantic at a random layer
#we do x = x + f(x) with the hope that x is self-sufficient in the sense of semantic (we are calling y semantic in the sense of f(x) -> y)
#that's the first assumption of f(x)

#we assume that f(x) is a sufficient approximation to snap x + f(x) -> diffracted euclidean semantic in the newly collapsed semantic space to do rotate
#why do I call linear context diffractor - linear is actually a dimensional reduction operation with the lhs being one dimensional rule and rhs being multidimensional coordinate (the arbitrary new cell in the output is now the dimensionally reducted information of the rhs)
#because we are in a one-dimensional-coordinate - we get the sound-alike effect - which is partially reduced by increasing the dimension size (the one dimensional space size is too big to get the sound-alike effect now - there is no neighbor - but is there?)
#that's the second assumption

#we are drawing lines - so it's hard to have a raw array of inputs <how, are, you>
#we need to boost this to 512 euclidean dimensions - to be more forgiving on the drawing wrong lines| planes| whatever - we are using a different radix of optimization to offset the inability of linear at this point (note that a massive blackhole could do whatever linear could do - in the sense of diffracting context - if we are at the event horizon)
#that's the third assumption - we are assuming increasing dimension size would be forgiving

#we are in a euclidean coordinate and doing basic math continuity compression methods
#that's the fourth assumption - our input - output (f(x) -> y) is not in a euclidean coordinate (take image processing for example)
#what does it take for the model to learn radix sort? this is the question

#after a long conversation with my machine learning friends - it's actually possible to use pure neural network + context diffractor to transport data without compromising the locality-ness of the data (we use pure math to alter the context to transport) or the data quality (this is debatable - we can't prove this yet - we actually can by using charges, assume there is an arbitrary space where the dimensionally reducted context is ..., prove that the euclidean relation is ..., with the dimension size = ...)
#the way from now -> there is a very long road and we might deal with numerical stability of differential methods - we'll come up with a way to solve this 
#but before we get anywhere - we must build a regex optimizer machine - by using centralities + path optimizations technique (because network network training convergence is differentiable)
#we are heading in the right direction
#with this rate - we are getting nowhere - we need to boost this to 1500 lines/day at least

#alright - let's talk about delaunay triangularization and the dentability of the continuity 
#we already talked about how tangent lines, charges and polynomials produce a unique footprint in terms of lossless compression in an arbitrary space
#we haven't talked about the continuity dentability of those methods

#charges and delaunay triangularizations and the distribution of the area | volume | whatever 4 dimensional "volume" - are important for differential methods to dent continuous strings
#if the area distribution (we discretize the area sizes 1.5m^2 -> 1m^2 for example) is too skewed - we are losing the continuity dentabilities - but we aren't losing the lossless compression (this is an important note) - because the charge coordinates produce a unique string footprint (somewhat a bijective relation to the string - we want to prove this by using flux)
#so the point of no return in "charges" is actually referring to the area distribution of delaunay regions

#maybe mathematical operations aren't sufficient
#but the idea of solving locality problems + context diffraction problems by doing contagious operation (message passing via the hardware closest neighbors + dimensional reductions) is the correct idea (in terms of computer science) - I'm not talking about differential or machine learning yet
#linear in large language models is more utilized as centrality algorithm than to draw relevant lines
#and it's really good at being a centrality algorithm
#we increase dimension to 512 - we map it to spacey one dimensional coordinate - and now we have a diffracted context (centrality) of the row

#what we are missing is actually row linear operations + column linear operations to avoid extra rotate operations
#we dont know what's more expensive - rotate then row linear or column linear, without loss of generality

#the question being asked is whether linear a good centrality algorithm?
#from the linear perspective, we are from a multidimensional coordinate, diffract context points -> 1 dimensional coordinates and do a rotate operation - we are assuming that our 1 dimensional representation is "nice" enough to represent the context of the multi-dimensional coordinate

#how about we have an operation to split multi-dimensional coordinate -> one-dimensional lossy compressed coordinate
#we need to consider individualism - such is each of the cell is responsible for an arbitrary space dimension of 512, without loss of generality - this is back to our way of sphere + explosion
#so how precisely do we do this? we diffract the context of 512 high resolution dimensions -> n x 512 low resolution dimensions (this is tricky - we need an associative operation (assume such operation exists) to solve the problem of numerical stability of rotation yet still being able to be lossless (if we associate all the uniformly splitted tokens)) - how to?
#so we have ourselves a nice centrality algorithm - we diffract the meaning of the row, we rotate, we mix - it's like a milk shake
#besides, we deal with delaunay triangularization and charges' distribution to further dent the continuous space - this is helpful if used correctly

#we'll work on the basic of centrality algorithms - we dimensional reduct the Earth (row) -> marble (cell) and shoot the marble (rotate) - and zoom it back to Earth size and do constructive interference of the Earths and the current Earth
#it's a milkshake operation
#we want to avoid the downsides of the one dimensional approach (euclidean context collision - by treating every cell as a sphere)

#there is a very narrow path for the centrality algorithm - such can offset the need for cuda - without loss of generality, by using multidimensional projections (as opposed to output cell one-dimensional projection) + rotate + shake + expansion + constructive interferences + add to the original now rotated context vector  
#we'll talk about the optimal centrality algorithm later - this has a lot to deal with individualism of euclidean space and resolution of the space

#what precisely do we know? we know that the basics of every intellect system consist of centrality algorithm, domain reduction and sort algorithm

#the centrality algorithm is the transformer architecture - imagine customized pagerank specifically for the input - output - transformer learns the centrality algorithm not machine learning linear that we know about (this is an important note)
#the domain reduction is hyperfocus
#the sort algorithm is another responsibility

#I have no proof for why centrality => intellect, and maybe intellect != => centrality
#thus a system that built on top of centrality and produces output based on input must be an intelligent system
#this hints us that we must emphasize individualism of nodes in an arbitrary tensor graph - we are back to the massful objects to do multidimensional projection (as opposed to linear being one dimensional projection) (x, y, z) -> (x1, y1, z1) and the single responsibility of euclidean space

#hyperfocus is the frequency mapping of the previous range and the current domain
#sort algorithm is the semantic reordering of the current domain

#I was having a coordinate confusion
#in the coordinate where the space ship is spinning - the net acceleration vector is pointing towards the origin
#so the ring is pushing us towards the center (we are inside the ring - not on the ring)
#and we are pushing back to the rim the same force

#in the stationry coordinate (where the space ship is not spinning) - the net acceleration vector is 0
#it's the coordinate calibration that generates artificial gravity - this is the confusion that I had  

#interstellar is actually a very insightful movie - except for when Mann tried to dock the spaceship to override the unauthorized operation
#alright - let's get back to the basic of centrality algorithms and context diffraction - goal is to be able to diffract + blend the context of the input matrix and the leaf matrix + dimensional reduct + transport + diffract + rinse and repeat
#the method for the context diffraction is unspecified - we can either do dimensional reduction of rules (one row, many columns) - which generate a synchronized operation of diffraction - or we can do dimensional reduction of rules + special rule for the focal - I've yet to think about this

#then there is string theory - goal is to be able to do calibration of space - to generate "artificial" gravity
#and we kinda want to rinse and repeat the massful objects operation of pulling or denting strings
#there are two kinds that we know about string calibrations - first is absolute string calibration - which is the one we mentioned - second is dynamic string calibration (such generates artifical gravity)
#three-body problem is about the dynamic string calibration (we're still working on solving this problem)
#string theory is very important to do context diffraction - we want the (x, y, z) -> (x1, y1, z1) -> (rx1, xy1, rz1) instead of (x, y, z) -> value

#then there is domain reduction (hyperfocus) - I'm still not sure about the specific implementations - just know that there is an exponential reduction of domain space - to "focus" on what's important - its like a wormhole to go to another space where important things are there 
#there is also a sort operation to make things "euclideanly relevant" - this operation is, without loss of generality, important before the tokenization step (which is translating the input semantic space -> byte_stream space or bit_stream space - interstellar perfers bit_stream for some reason, maybe because all computer arch is built on binary)  
#there is also a problem of delaunay triangularizations - this creates the anomaly of gravitational field which is a result of unbalanced area distribution of Voronoi diagrams (this is an unavoidable part of the training - we can only circumscribe the training area of the massful objects - not entirely eliminate it from disrupting the Voronoi's area distribution)
#we want to explore the "anomaly" (diving into the wormhole or the blackhole) which leads us to another space (which we already anticipated for)  where we can find the "context" to save the Earth

#it's a well-written movie for differential methods - I'm sure there are other methods to do centrality + string approximation I've yet to think about

#alright - the hinge was not quantifiable and the domain space is not compact enough for the range space (which creates anomalies - black holes - where we want to explore) - we'll think about this later
#I dont know math like my brother does - but I'm an experimentalist - I stick with path + heuristic + multidimensional projection rather than advanced linear - I really dont know what linear is for, except for learning purposes
#goal is to be able to use A* to increase convergence-ability of training - Dad talked about hinge, I agreed - the hinge is only not clear when it is not quantifiable
#what specifically do we quantify as hinges? learning_rate? cluser training sequence? logaritized value? mathematical operation? The hinge is ambiguous in the case - we probably want to build the basic information graph (with named entities) and build centrality algorithm on the graph to extract "the best possible next exploration, a.k.a hinge" from all of our users

#it seems that there are two graphs:
#the model graph - this is a tree of possibilities - each possibility represents a model (root - cursor) and its descendants represent possible paths of extension (cursor - base)
#the training graph - this is also a tree of possibilities in terms of training - each node represents "which group of logit", and the training rate for the group
#the model graph and the training graph intertwined and create a semantic space of named entities - which we can run centrality algorithm on to decide the best possible next "hinge" for our grand "A*" algorithm

#from our perspective - this is probably the way to create a dynamic model - such the convergence of loss_rate (d_loss_rate/d_t) is not linear (this is debatable - even if we exhaust all our "training intellect" - we would probably get something like 1/x d/dt curve which is f(x) = ln(x), this is success_line)
#there isn't a specific "fit-all" model - for the reason being there is not training friction (if we aim to generalize, we get nothing - that's the rule of compression)

#these are different radices of optimization - we have gravitational forces + delaunay + calibrations + centrality + domain reduction + focus + etc. as one radix
#we have path optimizations as another radix - this is hard to get right (we simply dont have enough users, yet)

#even if we implemented all of the mentioned things - we're prolly still in the radix of differential methods + centrality (which is a minute set of intellect)

#what is the optimal centrality algorithm? how do we actually define what's optimal and what's not? (we can't define what the optimal centrality algorithm is rather than to strategize and add the strategy into the regex optimizer engine)
#let's get to the basic - interstellar is right - the context window is fixed (we are only having that travelling input space, the spaceship) - so we must leave something behind in order to get something new (new context) - we need to escape the pulling force of the black hole in order to stay contextually intact (by using propeller, attention, WLOG, described 2014) - otherwise, everything sounds the same (the blackhole is no longer a context diffractor but now a heavy influencer of what's the black hole means)
#note how the spaceship leverages blackhole gravitational force to have free ride - if the a = v^2/r - we are basically having a free ride around the circumference
#we can actually leverage the v^2/r centripetal force to do "sling-shot" without compromising energy (the spaceship is stationary in an arbitrary calibrated coordinate)
#then the spaceship found it place where "other spaceships" haven't been to yet - initiate propeller to escape the free_ride (we are doing context projection + string exponential - (x, y, z) -> (rx1, ry1, rz1) and ((x, y, z) * (x1, y1, z1)) '^' n)
#alright - the numerical stability of the "exponential" part is the hard problem to solve - we break numerical stability very easily there - yet we are back at the contextual space and the law of information storage - maybe differential methods can't be used to solve numerical stability (note that numerical stability is hardly a problem - moving in the differentially "right" direction is the problem)

#what are the planets in interstellar? They are brains - with waves of backward + forward propagations
#too watery would be an unsurvivable planet - or too dry would be unsurvivable as well
#note how Dr. Brand tried to collect the information before the wave - and got caught in the wave (the memregion_lock) - and the spaceship got halted for years (this is the funny part - because the frequency of the wave on the planet is way slower than the frequency up there in the spaceship - planet is too heavy)
#interstellar tried to warn us to avoid to operate on the region at its backward frequency (we'll come up with a way to solve this) and try our best to be "synchronized" with Romilly (imagine we are back when Romilly is still in cryo - and not knowing anything about the blackhole)
#moral of the story: try to catch the information in time - if not - get out before Doyle's dead because of timeout

#note that when Cooper fell into the blackhole, all he saw is a room - this means a context skew happened that disrupted the Voronoi diagram area distribution (but this is a very important room where everything happens - thus the skew - and this "tesseract" denotes individualism of euclidean space - this is an important note)
#alright - so centrality by using "spaceship" implementation requires us to have the art of giving - transactional - we give something bad, we get something better - we diffract the context (notify everyone) - we rinse and repeat  
#how did Cooper communicate with his daughter again? by pulling strings - so the constructive interference operation interstellar used was string "multiplication" operation
#after his daughter got the neccessary information - problem solved (f(x) -> y) - mission accomplished - Cooper is rescued
#note that the daughter was awared of the watch - but didn't know the context of the watch - until certain time kicked in which initiated the training sequence involving the watch (to do context extraction) 
#this hints that the training order of dimensionally reducted context is important - most likely post the Voronoi's anomaly had been formed and another domain - range space with sophisticated output had been constructed
#so the dedicated buffer is there - but the leaf logits that involved the buffer weren't trained yet (I'm not very sure of how to achieve this technically - because good centrality algorithm is hard to write, let alone leaving a restricted region untouched)

#the way that interstellar wrote the information is very insighful - we do domain reduction - domain expansion and do string operation on a preallocated designated buffer - hoping that his daughter (in another domain) would notice
#alright - so the taken approach was appearantly waiting for the Voronoi area skew to happen -> construct another domain - same goal (range) -> collaborate by writing-back to avoid skewed training problem
#my approach was slightly different yet harder to achieve - we preallocate + construct exponential domain reduction - do real-time analysis of what's needed to be reducted (by using frequency of euclideanly related fired together + discretize + sort) -> map + offload responsibility of output resolution to another domain (we break numerical stability here if the domain collapse rate is not fast enough - and the real time mapping is not stable)  

#the rescue operation (constructive interference) of the Endurance was an advanced operation - we use a spaceship to save another spaceship - by calibrating with its contextual space 
#this hints that constructive interference is only good if calibrated - otherwise it'd have destructive interference for both of the spaceships

#let's talk about the string ^ n operation
#we need to iteratively compute the artifical gravitational points to calibrate + construct another stationary space where we could keep the numerical stability post string ^ n - and maybe we aren't using differential methods to "heuristicly" guess the "gradient" for string as in string ^ n (we need to have memorized previous results because the possibilities for string only converge in a non-differential + accumulation way - maybe this is temporal fired together grouped together responsibility)
#this is hard - and another entire research topic to think about (we can only think of memorized x (x1, x2, ..., xn), and the reconstruction of the string by using non-differential methods)
#we aren't worrying about this yet - this is an advanced topic - calibration is another also very advanced topic - it seems like we extract the "^n" of the operation to do coordinate calibration - this is precisely the dynamic calibration of the Cooper's spaceship (row) and Endurance spaceship (rowxcolumn), Cooper spaceship - without the string(^n) or spinning in the endurance direction - we are having more destructive interference (explosion) than constructive interference of contextual space

#why is the worm hole a closed string again? because continuity is better described if closed (a circle, a sphere, etc.) - and I approved of this approach

#well that was a ton of implementation - but let's talk about the core core, mandatory recipe of everything - it's centrality - (input) => output - everything that is centrality-related is intelligent
#the string^n is actually also a form of intellect - besides centrality algorithm 
#we want to do self attention by using string ^ n operation because the rocket fuel is from the spaceship - we detach the rocket fuel - and we do self expulsion forces between the fuels and the spaceship (which is a closed system) 
#so we want to do string ^ n to have a good sling shot around the Gargantua - then we do self expulsion (attention) by splitting the spaceship into two parts (the fuel and the spaceship) then we do self expulsion of closed system (the fuel and the spaceship) without loss of generality
#string ^ n was also a without loss of generality, we want to estimate a reasonable x by using projection to narrow the numerical stability (keep it in control)
#that was the best description of context diffraction + context blend + self attention - well done by the interstellar cast

#we've yet to understand why centrality is intellect (this is the easy road of achieving intellect) - the complex road is string exponential - we'll be there some day
#and we don't know why two centrality graphs should be sufficient - the training graph (unhinging) and the model graph (the computation tree)
#yet that would be our thesis for probably the next year
#we'll try to reach escape velocity of intellect - this way or another

#the semantic space calibration is actually important - and the algorithm to calibrate the thing is hard to implement
#takes computer for example - we stroke a key - it appears on the screen - but behind the mechanism - there is the device buffer (monitor_buffer), the semantic sense maker (operating_system), the hardware transportation system - the electron - the God's language, etc.
#the layer of semantic is built one on top of another - if it's not calibrated correctly - the centrality algorithm would render useless

# //now let's talks about computer science and the calibration of the layers
# //first is the hardware instruction fetch (semantic collapser) + transporation layer + buffer
# //second is the first program - an event loop
# //third is modern languages + operating system
# //fourth is virtual machine

# //how exactly do we do calibrations in centrality? This is a hard task - alright - is calibration in centrality a term for another word? says that we can't really calibrate - so we build another word for the fired together (which now has a separate render rule)
# //alright - note that the calibration decays very quickly - maybe 10-20 depth max from the original semantic - we need to offset this cost

# //now is the advanced topic - we are 20 years past centrality and now we wonder how to do calibration in string approximation
# //let's talk in terms of gravity - how did interstellar do calibration? there are angular calibration, artificial gravity calibration and three-body system calibration

# //let's say continuity compression can be solely described in terms of gravity manipulation - then our only concern is the reference frame of the gravitational operation
# //and the process of adjusting the reference frame is calibration
# //recall the planet that is on a curved plane - where did the ball hit? a window 

#says we already know the tricks - all the reference frames and their friends - how about we build a system that involes the calibration rules and friends and let the system figure out the way to make sense of things?  
#thing is we don't choose the things to be calibrated - we assign the things to be calibrated - and we train the system based on the rules 
#let's talk about the big picture
#we have galaxy spins around the center of the universe
#the Sun spins around the galaxy
#the planet spins around the center of the Sun
#electron spins around the neuclei
#do we actually assign what spins around what in runtime? No - because these are preassigned - and according to the hierarchical order - it looks like a tree with outdegree - such is the descendant (whose spins around the ancestor) must use the ancestor to do reference frame calibration 
#alright - let's move to the generalized form - it's B Tree of calibration - this is the dynamic preassigned calibration (which is described in the interstellar movie)
#this and the centrality algorithm are two different spectrums + radices of approximation

#artificial gravitional calibration is the process of converting centripetal force (or acceleration) -> gravitational force - such is the origin is stationary in the reference frame
#how about the three-body calibration? it seems like the three-body itself is a problem and the "one of the bodies" is another problem
#we want to calibrate on one of the bodies such is the body is stationary in the reference frame
#how does this relate to the B-Tree? alright - the B-Tree cell consists of many nodes - which is a subject to closed system projection - and its descendant is a subject to origin calibration
#how do we train this thing without breaking numerical stability? alright - this is the question that maybe only God could answer - let's start from there must exist a heuristic such that the required training data size must not exceed the semantic space size
#                                                                                                                                 - and the differential progress might not make senses if it exceeds the training uncertainty  

#alright - we are greedy - how about we combine the tricks - we run centrality on the string approximated system?
#don't worry frens - we'll carry the team - we'll show the naysayers about the true power of 2 centrality + hyperfocus + string approx + compression + sort
#we probably would break through the current security layer (JWT + https + symmetric + asymmetric + whatever) with those 5 methods, if implemented correctly - and reach escape velocity
#1980s is a good time to rewind

#what I agreed with Dad is Machine Learning is a tough subject to write - not because it's tough - but because of people trying to prove machine learning by using math + customizing their models
#what us engineers must do is try to explain machine learning as paths + optimization technique + strategies
#because otherwise, we are getting pure luck and not doing actual engineering but rather guess and hope for the miracle from his benevolence

#in Machine Learning, we add strategies into our optimization machine like numerical stability, skewness of training, regex model, etc.   
#if one model is not enough - we'll try to add two models - if not then three models - model learns model learns model
#the what to do next in machine learning is actually a path problem - what model, what tile_size, what data_type (bool, float8_t, float16_t, float64_t), what training data, which tensors, what learning rate, etc. 
#this path problem does not have a clear hinge like A* or dijkstra or BFS or DFS or floyed - yet the hinge is from another centrality algorithm

#what does it take to reach escape velocity? I really dont know at the moment speaking - rather than binary string + calibration + hyperfocus + centrality of centrality + spaceship operator + compression + sort 
#if we implement all of those correctly - I haven't heard of an individual that is capable of this yet - given the complexity of the task - this would take a team of 50 people at least 5 years to have those correctly implemented
#yet it is not impossible - I admit things are hard - yet all of these, if I've calculated correctly, should take 2 million lines of compact, specialized code to run - it might take me years

#after a series of experiment, what we realized is that an attempt to approx a dynamic system (tree | path | centrality | gravity calibration | etc.) - is intellect - and the focus on what's important (domain reduction + words + blackholes + wormholes + etc.) is important for differential training 

#recall the A* algorithm, we have heuristic of reaching the destination, and we have actual cost of reaching the current state - and we have state expansion and a priority queue to unhinge the next move - what we are unsure is "the hinge and the heuristic"
#recall how we assign our gravitational calibration points - now we assume that we have hinges, we have path, and we have an admissible heuristic - and we train a system based on such - alright so what are we finding again? we are finding the "Google Map" to run A* on (this Google Map has a single destination - right or wrong)
#how precisely? recall that an admissible A* algorithm would reach the minimum possible distance after k moves - we want to find the standard distribution of k - and move k to the 90% percentile and find the minimum hinge and backprop that hinge 

#the correct 21st century algorithm is a combination of those strategies - we dont know if it is centrality + path - or centrality + gravity + path - or all of those strategies
#we just know that it must involve path + centrality - recall how we do the floyed algorithm and try to "relax" the responsibility of a node (it's an advanced parallel algorithm - a radix of dense centrality algorithm (sparse centrality is linear) - we must be able to approx the floyed algorithm to map the semantic as in f(x) -> y)

from typing import Callable

class Particle:

     def __init__(self, mass: float, position: list[float]):        
        self.mass = mass
        self.position = position 

class ForceVector:

    def __init__(self, vec: list[float]):
        self.vec = vec

class String:

    def __init__(self, string_particles: list[Particle]):
        self.string_particles = string_particles

def get_multidimensional_string(dim: int, resolution: int) -> String:
    pass

class StringOperationTree:    

    def __init__(self, left_string_tree = None, right_string_tree = None, operation_kind = None):
        self.left_string_tree   = left_string_tree
        self.right_string_tree  = right_string_tree
        self.operation_kind     = operation_kind

class OpsString:

    def __init__(self):
        pass 

OPERATION_KIND_SELF = 0
OPERATION_KIND_MUL  = 1
OPERATION_KIND_ADD  = 2 

#alright what is an operation tree? an operation tree computes the string(x) -> y
#operation tree has string * string (which is massful influence) or string + other string
#string * string is done by combinatorial pairwise operation of lhs on rhs - such is rhs moves towards lhs by the eqn f(pos, r)
#string + string is coordinate calibration - we calibrate the string by doing + operation - alright - assume we discretize an arbitrary string est surface into one dimensional pointer, then for every iteration point the new_point = sum(distance(O, i) for all i c iteration_grid)
#assume we are on radian coordinate - we have the initial space of a sphere (which is shape)

def clone_tree(root: StringOperationTree) -> StringOperationTree:

    if root == None:
        return None

    return StringOperationTree(clone_tree(root.left_string_tree), clone_tree(root.right_string_tree), root.operation_kind)

def make_operation_tree(dim: int, semantic_layer_height: int, tree_skewness: float, total_node_sz: int) -> list[StringOperationTree]:    

    if total_node_sz == 0:
        return None 

    if total_node_sz == 1:
        return StringOperationTree(None, None, OPERATION_KIND_SELF)  

    left_node_sz: int                           = min(max(1, int(total_node_sz * tree_skewness)), total_node_sz)
    right_node_sz: int                          = total_node_sz - left_node_sz
    left_tree_list: list[StringOperationTree]   = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, left_node_sz)
    right_tree_list: list[StringOperationTree]  = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, right_node_sz)
    rs_tree: list[StringOperationTree]          = []

    for i in range(len(left_tree_list)):
        for j in range(len(right_tree_list)):
            new_tree_1  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_MUL)
            new_tree_2  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_ADD)
            rs_tree     += [new_tree_1]
            rs_tree     += [new_tree_2]

    return rs_tree

#these are the three string operations that I think sufficient for every semantic approximation
#calibration, massful curvation + projection

def add_string(lhs_string: String, rhs_string: String) -> String:

    new_particle_pairs  = list(zip(lhs_string.string_particles, rhs_string.string_particles))
    new_particle        = [sum(pair) for pair in new_particle_pairs]

    return new_particle

def mul_string(lhs_string: String, rhs_string: String, eqn: Callable[[Particle, Particle], ForceVector]) -> String:

    new_string: String = String()

    for j in range(len(rhs_string.string_particles)):
        force_vec: list[ForceVector] = [] 

        for i in range(len(lhs_string.string_particles)):
            force_vec += [eqn(lhs_string.string_particles[i], rhs_string.string_particles[j])]

        new_particle = move_particle(rhs_string.string_particles[j], combine_vec(force_vec))
        add_string_particle(new_string, new_particle)

    return new_string

def project_string(string_projector: Callable[[String], String], s: String) -> String:
    pass 

def main():
    #I'll be back to write this tmr
    pass

