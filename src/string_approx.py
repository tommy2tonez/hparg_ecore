
#alright - this is string approx - string approx problems can be defined as - stretch horizontally, stretch vertically, periodically (this is 1 - many linear (gemm)), and calibration

#let's build a tree of string operations
#string operation is essentially massful object interference on the surface - to pull the surface towards the massful object 
#let's assume our base is f(x) = 0, we have 1000 points on the surface - each massful object is going to pull the points above or below the surface (towards the massful object's coordinate)
#we want to recursively build our continuous function by this formula

#f(x) = f_previous(x) + string_interference(f_base, massful_objects)
#p(x) = string_interference(f(x), massful objects) 

#let's get to the root of intelligent semantic space - intelligent semantic space is calibration + locality compression (continuity) + periodic (periodic by dimensional reduction)
#we aren't doing random semantic space
#alright - you have a continuous function f(x) = x^2 (string) - it does not mean that it has square relation on the training data f(x) and x - but it is in some coordinate
#so it's important to do calibration (which is p(x) function) of coordinate as we are doing training - this is an oversimplification
#assume we have 20 base strings that make up the projection space 
#the first string will get calibrated 19 times (via the usage of massful objects)
#the second string will get calibrated 18 times
#the base string is our arbitrary "calibrated coordinate"

#the art of compression is machine learning - yet I think we are not there - the calibration part and the periodic - we got the base string correctly
#we need to work on the theories
#every machine learning problem is to approx the f(x) -> y projection string - by using continuous compression
#whether you are linear absolutists or string absolutists - we want to dent the continuity - and recursion building of context space (how we get the recursion building correctly is described below via the trinity tests) - and repetitive rules of continuity  

#gravity is probably on the 21th string
#electromagnetic is probably on the 20th string
#pay close attention that electromagnetic is affected by gravity but not otherwise (this is a hint that the massful_objects for electromagnetics are probably of lower string hierarchical tree)
#we call string a multi-dimensional projection shape
#this is a huge advancement in string theory - because it tells us that things, actually, can be created from multidimensional hierarchical strings (which is what transformers had been able to prove)

#think of the massful objects as semantic rules
#the first layer of particles (base_string) has to adhere to 20 semantic rules
#the second layer of parciles (base_string) has to adhere to 19 semantic rules
#etc. 
#this creates training friction for the 1st layer 2nd layer - such that the training must be able to find the semantic rules for the 1st and 2nd and 3rd layers in order to proceed
#note this very important rule of machine learning training - we need to create training friction

#on 2D euclidean coordnate - we have particle projection - and we have wave-like projection which is a 2 dimensional string
#thing is we want to use God's favorite language of "massful" for continuity compression
#what Einstein described was 4 dimensional string - and massful object would bend continuity of the 4 dimensional projection string
#Einstein was not wrong - just not sufficient - in the sense of information flux
#information flux is the maximum information that an enclosed space can receive from outer space (which is kinda maximum pingpong/s) - alright - we are talking in terms of 4 dimensionalists - how should we describe this is terms of 20 dimensionalists? it's resolution - saturated logit problems
#string quantum is right - but we can't observe another world to tell our world - the photon that went through the slit and the observed photon aren't on the same coordinate - they are on 20 dimensional projection string - and they went different direction  

#so what's our regex of building the universe? or compress continuity? Or at least this is the God's language
#f(x, n) = string_interference(f(x, n - 1), massful_objects) + f(x1, n1)
#f(x, 0) = base_string

#alright - let's twist things - God wants to confuse us - assume that we add one dimension (which is massful iterator) for massful objects - and increase our dimensiona from n -> n + 1
#what happens?
#each of them massful objects is on a different timeline
#pay very close attention to how we could simulate the universe (in our point of views and how we can differentiate the x and f(x) - hint they are all (massful objects or logits) started from the center - the Big Bang)
#all right - we are missing calibration projection - if we are string absolutists - we probably dont need calibration projections for now (because it messes up numerical stability)

#if we are absolute theorists - we must include string calibration projection
#assume the famous three-body problem - we want to be able to project the position of the three body on the 3d vs time coordinate by using stationary massful objects (on an arbitrary coordinate)

#we first have initial velocities of the three bodies
#we have the masses of the three bodies
#we have the positions of the three bodies
#we have the t0 of the three bodies 

#we first have static massful objects to rid of the mass + initial position continuity influence - and we are left with initial velocities, position and t0
#we then have static massful objects to rid of the initial velocities influence - etc.
#we then have ... to rid of the time influence

#without loss of generality, we "collapse" the semantic space to be able to approx things easier, better, faster
#the first layer already got 80% of the shape correctly
#the second layer got 10% of the shape correctly
#the third layer got 5% of the shape correctly 
#we want the hierarchical build up to allow training "friction" of strings - to simulate the actual semantic collapse and recursive build up of semantic space

#i've been thinking about string projection  
#because it is important to reorder the input domain to do continuity of different groups - without this, we are logic-impaired (in the sense that we always assume the domain to be of continuity)

#let's do things simple - calibration - massful influence - and trees 
#we want to be able to approx 3 body problem, murmur hash output space and sin cos interference as proof of concept (this is the baseline of every semantic approximator system - periodic, recursive buildup and constructive interference - if we can't get this right then I'm afraid we aren't going anywhere)
#we'll get things done today and tomorrow - then we think of how to scale this thing

#alright - we've been talking about string and projection - let's simulate string in projection environment
#assume that we are not flat-earthers
#we wanna ditch linear for massful operation

#we want to assume that we are the one element in the invisible string
#assume that we want to simulate two dimensional string
#we first want to have a linear operation - f(x) = <x, x>
#we then want to have a massful operation mass = <x1, y1>
#a pull operation should transform <x, x> -> <x + (distance * heaviness) * cos, y + (distance * heaviness) * sin>, distance = sqrt((f(x) - mass_coor) . (f(x) - mass_coor))
#alright - let's do the reverse of Chad soy boy
#assume you have the continuity of the universe, we want to unhinge the universe layers by layers
#assume that the universe is made of invisible strings and there are massful operations to bend continuity 
#what we want to do is exactly like transformer
#x = x + peel_layer(x)
#get_context_layer is essentially the, without loss of generality, (distance * heaviness) * cos, (distance * heaviness) * sin, etc 
#universe is actually like an onion - we want to peel layer by layer - by using tons of massful operations for each layers
#thing about linear is precisely that - imagine that we are on 3 dimensional string projection - we are drawing plane to peel onions - which is (1): drawing lines between irrelevant pts, (2): cause bad projection ripple effect which has bad numerical stability, (3): only good for parrot affect - sounds similar but not the actual context 

#alright - let's talk about how to build a model
#thing in machine learning is there are only two layers that we should talk about - the input layer and the output layer (because machine learning is all about numerical stability - and not about what people say)
#we want to describe the output layer based on the input layer - and how the coefficients move the output projection
#we dont really care about the middle layers and what it really does
#if we only have linear operator and we rotate the intermediate layer correctly 
#we expect y = a * x1 + b * x2 + c * x3 + ..., for y is a random output cell

#if we have self operation - we expect to see polynomial
#if we have attn - we expect to see exponent - which touches the points that the linear could not touch
#alright - how about we describe things in terms of string?

#how about we have embedding f(x) = 0 for every layer? - for x is the initial input (before embedding projection)
#and we apply the massful operation
#so we should expect to see things like f(x, n) = f(x, n - 1) * massful_objects + f0(x) * whatever
#what is wrong with this?
#that there is actually nothing wrong with this - we don't have self operation to increase polynomial order or exponent to increase the touchness of the graph
#we encode all the information in the massful_objects
#we assume there are invisible strings - and we build strings on top of each other to simulate context collapse - and we want to work on numerical stability of this  

#alright - we always want to start from - what we expect the output layer to be from the input layer and we build things from there
#I say I want the output cell to be in <x1, x2, ..., xn> space and we are able to bend the string space f(x) -> y projection - layer by layer like the formula - ((embedding(x) * massful_objects) + embedding(x1)) * massful_objects   
#alright - what are the other things that we see - we see recursive building of things (alright - attention does allow recursive building of things - because without attention the layers aren't making senses) - which we want to prove that works (or we want to twist things to allow this to work)
#what other things that we see, that embedding(x) and embedding(x1) probably look the same - and we want to reuse the massful_objects rule for all other cells

#we gonna gucci later babe - just have faith for probably 2 years - I'll try to code the regex + path_optimization + randomization + convergence analysis + derivative of gradients + etc.
#dont trust Chad - all he does is offensive meme and closing things

#what if I tell you that AI is three body of three body of three body of ... problem
#let's investigate the three body problem https://en.wikipedia.org/wiki/Three-body_problem
#it's https://en.wikipedia.org/wiki/Puiseux_series, proved by Finnish mathematician Karl Fritiof Sundman - not Linus Torvards
#alright - we have f(x) = puiseux_serieries
#we probably want to do f1(f2(f3(x))) - this is where we are at - the transformers and their exponents

#how about law of momentum conservation?
#alright - so what's momentum? momentum is the moral of the story behind the forever spins around each other of the-three body problem
#God - the old man - does not have infinite resource - he needs to store the moral of the story in order to bend continuity
#in other words, he needs to have story coherence in order to do story telling of what's next in the continuity story
#back to the gymnastic story - a girl spins in a complex motion - but there's no moral of the story for continuity  
#how do we actually incorporate this in mathematics? - could this "moral of the story" be described in mathematics terms? 
#yes - but it would break numerical stability of the result - I'll be back to solve the problem today - 

#finding the momentums cannot be done via differential methods - we break numerical stabilities when the uncertainty exceeds the gap closing of gradient approximation
#linear, eigen values and eigen vectors are cool - but what happen if we do string multiplication?
#back to the gravity problem - f(t) = string ^ t
#there aren't many ways to do this correctly (at least not the differential roads) - but the f(t) has a unique footprint that narrows the function down to only a few operations to approximate (we are approxing string) - alright - we are back to the decoding problem and semantic space + information storage theorem
#maybe differential and Newton are only good for approximating classical local optimal solutions
#alright - this works only if we do precalculation to do heuristic pruning of what's possible version of string - we want to discretize + precompute this - because this is a very important concept of neural network - the dynamic continuity
#
#so we have changed our coordinate to radians - and we have all the initial string right in the middle of the coordinate (the origin) which has a sphere shape - and we have a sphere shape of massful objects in the outer layer, we are doing an enclosed shape of continuity (this is an important concept) 
#thing is this way of doing things have a very smooth curve of gradient - and we are not stuck in the gradient valley like polynomial methods 
#and this actually simulates the actual context collapse efficiently - the recursive build up - this is up for debate - it only is if the projection string is kinda round - not like zig-zag shape which affects the uniform distribution of massful objects which require a string projection in order to rebalance the uniform responsibility - or skewed responsibility  
#we want to mess with numerical stabilities in the newton approx, and we dont want to stuck in the valley - so it's the way of doing things
#alright - let's talk about the numerical stability - we have two spheres
#Dad talked about the gradient valley of curves - such is there is a non-returnable point of shape - such is the "hardness" of shifting from the shape to another possible shape from the original point is impossible
#we must circumscribe the closing circumference gap (middle theorem) to limit the non-returnables, and we kinda stay in the acceptable fluid zone of gradient moving 
#so the deeper the valley - the harder it is to shape the curve - or the distribution of possible shapes is skewed in the direction of the valley
#so we must do string projection - to "normalize" the input string - and keep it in "acceptable" gradient zone

#in a general perspective, we have f(x) -> y is projection string
#we have possible shapes - and we have the possibility of moving from one shape to another (we kinda want to flex this area of possibility to not stuck in the gradient valley)
#we have string altitude normalization - we normalize a string to do shape bending better - as described above
#we have string ^ t heuristic approximator - we need special method (not differential) to converge this approxmiation
#we need to make sure that the uncertainty of gradients and destructive interference do not exceed the learning
#things gonna be hard but I think its possible
#this mimics how our brain actually works
#whenever a logit moves - we move towards the result - but losse the old information - the question is given the appearance of the results in a certain window - what's the window of uncertainty of the not appeared results? How fast do we actually forget things to burn new information?
#a fully burnt brain does not move logits as fast as a newborn brain - in the way that a logit move does not touch all the projection string by r^2 or r^3  
#and the lower layers are actually very important to predict | train the upper layers - given a math semantics x^2 + y^2 + z^2 - our brains collapse the math semantic space and do string projection to a new space where we could do another semantic collapse without affecting the old space - things get confusing here
#if we mess up the lower layers - we steer away from the correct path - and the incorrectness might diverge - not converge
#in order to make the semantic collapse "real" - we must do string normalization - assume our example of math semantic collapse as murmur hash
#in murmur - we shift, multiply, rotate, etc - when we shift - we move it to another semantic space - we multiply in the new semantic space and we rotate in another new semantic space
#good string normalization and numerical stabilities are hard to do

#we probably can mimic the string projections by calibrations - this is hard - without actual string projection we are bending continuity of irrelevant data
#this is hence the f(x) = x + peel(x)
#how do we actually describe this in mathematical terms?
#imagine that we have x y z on the slot 1, 10, 20
#we want to reorder the x y z to 0, 1, 2 - and bend the continuity there - because they are relevant terms in mathematics - and we want to map it back to 1, 10, 20 and do a constructive interference operation - that's kinda the idea of context space collapse of euclidean irrelevant data
#this is a position reordering operations
#position reordering operations (or calibration) are actually the tough part - do we do mapping based on set frequencies? or continuous functions? things that appear together should be grouped together - this is part of the temporal linkage part of our brains - things fired together linked together  
#a simple linear(x) is too vague to describe this functionality
#how about a discretized tree of semantic space - things fired together are interval tree responsibility

#alright, all the difficulty is because of we want to group relevant things together - things fired together - we map it to another semantic space (and now we have another things-fired-together) - we do continuity bending there - because they are relevant - we map it back to our semantic space - we do constructive interference
#and we group those fired together things again - we keep doing that thing, which is called semantic space collapse of irrelevant data - look back to our example of murmurhash - this is not a necessity if the semantic space is nice and round

#our state of the art is actually not that we do context collapse (which is briefly described via the solution to murmur hash approx) but the reordering of context space - to bend euclidean irrelevant things together
#assume this simple model of doing so
#given that our input layer is the base
#given that we discretize the space into grids
#given our next to base layer is the layer of the things fired together in the base
#given our next to the next to base layer is the layer of things fired together in the next to base
#what do we want to do with this? for every layer not base - we can do an inorder-search of base to create a newly-reordered semantic space without breaking numerical stabilities
#the attention was to solve this specific problem (the problem of bending euclidean irrelevant things) - but in a numerical instable way
#the actual model of reordering must be of complex forms - and involve algorithms and not maths - we'll talk about this later - this is hard

#alright the topics we are going cover today are hyper focus subspace + discretization of euclidean space + space dimensional reduction + dimensional expansion
#we have talked about the things fired together grouped together by temporal linkage - all linked to a master node - which is again fired together and linked to another master node
#we then have our newly created semantic space in terms of graph - which represents temporally relevant things
#but we want to bend this this in euclidean coordinate (because we have a well establised string theory of how relevant things bend together) - so we must invent a way to convert graph semantic space -> euclidean semantic space - by, without loss of generality, the formula euclidean_distance(node1, node2) = shortest_weighted_graph_distance(node1, node2) or maxflow(node1, node2) or by whatever lossless compression that we can reconstruct the semantic graph from
#alright - things aren't perfect - we can only fuzzy approx the euclidean coordinate - because there is no perfect answer
#how about we do it again? rinse and repeat in our newly created euclidean coordinate

#discretization of euclidean space is nothing but interval (in terms of for loop) - we have intervalx(intervaly(intervalz)) - and we grid the (dx x dy x dz) 

#let's cover the space dimensional reduction | expansion in a nutshell
#let's get back to the example of x y z
#assume we are on three dimensional domain - the chances of x y z "euclideanly" collaborate are higher than on one dimensional domain (because we can move in the 2nd and 3rd dimension to relatively bend things without compromising the euclidean_distance(node1, node2)) 
#what's the difference between 3 dimensional domain and 1 dimensional domain - there is essentially no difference if the one dimensional domain is domain(x) * domain(y) * domain(z) - which has bijective relation to the 3 dimensional domain
#the only difference that we are mentioning is the euclidean distance - which is important for bending strings by our described methods
#so increasing dimensions helps continuity bending of relevant groups

#let's get back to our tokenization methods - do we enumerate the vocabularies to create an arbitrary semantic space - or we do byte stream - or we do bit stream
#bit stream is the ultimate goal of information technology - because all the algorithms mostly operate on bit levels
#alright - so we increase the dimensions of the semantic space - which is, by our definition, a good thing - because it increases the groupabilities
#so what's hyper focus - hyper focus is the abiltiy of temporal linkage of tensors - recursively to create an arbitrary semantic spaces to increase the bendability of relevant tensors  

#how precisely do we build hyper focus? by using weight decay on edges and broadcasts from input nodes and temporal group of lit up nodes (weakly connected component)
#so hyper focus is a centrality algorithm - and linkage of lit up nodes is community detection algorithm
#alright - we need to have decay rate of community and edges because we are on finite resources
#the only responsibility of hyper focus is to reorder the semantic domain (community detection) of relevant things (in terms of input space)
#and we want to construct a euclidean semantic space from the hyper focus graph space to bend continuity there
#and we want to reconstruct the hyper focus graph space from the bent euclidean space to update the hyper focus graph space which will be reflected on the original euclidean space
#or we can adhere to the formula of euclidean_distance(node1, node2) = maxflow(node1, node2) - and we have bijective domain relation to the original euclidean space - which we can directly update on
#hyper focus and string continuity operations are two sides of the same coin - can't exist without another - unless the projection string is round 
#we'll work more on the theory
#our grand plan is to reduce the complexity of constructive interference of n! space (n is the domain's range, n c N)  -> 32 reordered space by using hyper focus with the hope that hyper focus would bring relevant things together to help our continuity bending

#consider this example
#yes, I'm fine
#yet, I'm fine

#alright - yes, I'm fine and yet, I'm fine are euclideanly related - in the sense of sqrt((coor(s) - coor(t)) * (coor(s) - coor(t))) - but the semantic is very different, this denotes a requirement for reordering of the euclidean domain grids - otherwise we are bending irrelevant context (which is against continuity bending of relevant context)
#we solve this problem by using lit up tensors community detection - yes, I'm fine is categorized as good signal - yet, I'm fine is categorized as bad signal in euclidean tensor graph - and we want to recursively reconstruct the euclidean semantic domain space from the graph temporal semantic space - we can't actually eliminate all the false positives - but we kinda "commnunity detect" potential context relevant things from temporal relevant things which help continuity bending
#the art of n! reorderings -> 32 reorderings requires a lot of work - not a simple attention layers - if we are seriously talking building a brain 

#let's talk about dimensional reduction more in depth - we have input domain - we have temporal semantic graph - we reconstruct the domain from our graph, now our newly created domain must spans higher space range - because it encapsulates more information than just the original domain
#let's say "how are you" and "how are you doing" are temporally grouped, without loss of generality
#they are in slot 0 and 1 respectively in our new domain - but the slot 0 and 1 are only 1 bit of information - so we have reducted our domain resolution from bit_size("how are you") + bit_size("how are you doing") -> 1 bit
#the reconstruction of the domain is necessary for our constructive interference - which we will talk later - we are talking in terms of string and not tensors for now

#let's talk about our brain
#let's say we have 5 senses - which orderly spit into a matrix
#our eye tensor inputs are euclideanly closer than our ear tensor inputs
#but not euclideanly closer means semantically relevant (back to the example of yes and yet, yet this is actually semantically relevant in terms of brain mechanics - things that are euclideanly closer are easier to establish linkages - and learnings) - we must do temporal reoderings of tensors - map into another domain and do continuity bending - and we rinse and repeat until we get the result
#note that there are two very different systems in our brain - the system of temporal reordering of input domain and the system of string bending - the system of string bending is described via the hashing method - the system of temporal reordering simply just observes the lit up tensors and do centrality + community detections and reconstruct the input domain
#the proof of concept is done when we can approx murmur hash via string multiplications and efficient reordering (I cant tell the criterias for what "efficient" is for now rather than "maybe temporally related means maybe contextually related")
#side note: we are being confusing when we mentioned lit up tensors - what we actually meant in the context is lit up domain grids, and tensor is just a grid pointer
#this's gonna be hard - but we'll be there in probably a week or two - stay tuned
#theory: if the internet is 1 petabyte - then the most efficient learning system only takes 1 petabyte of learning to reconstruct the semantic space
#the most compact semantic space size can not exceed all of its possible outcomes size

#let's talk about specific implementations
#there are few things to get straight - first is string continuity approximation by our formula f(x, n) = string_inteference(f(x, n - 1), massful_objects) + f1(x, n) + three body system formula - we are assuming that closer euclidean distance means closer in semantic in this fixed coordinate - this is a very important assumption that we must be able to achieve via hyper_focus system
#hyper_focus brings domain grids together from euclidean irrelevant distances + fuzzy ordered set context of grids + dimensional reduction to not explode the domain space
#in order to do so - we must do lossy compression of fired together grids (otherwise we are OOM) - bring them into an intermediate semantic space where contextually relevant things would collide at the same address
#those temporal semantic space of fired together grids again reconstruct the semantic space domain
#now we do constructive interference of the f(x) + f1(x + f(x)) - for f is the old semantic coordinate and f1 is the new semantic coordinate
#we are hoping that x + f(x) would reduce the semantic space size as we proceed - because fired together grids entropy must converge - otherwise - we are reordering the domain space for ... nothing - because we are not hyper-focusing
#how about we build the hyper_focus by using interval trees? somewhat like our heap implementation
#we discretize the domain - we build an interval tree with a reasonable memory allowance - and we connect the tree to the "hyperfocus" system where we could reconstruct the domain and build a new semantic space

#without loss of generality
#how about building another heap, on every level of our heap is a newly collapsed domain of the immediate next level

#so the specific implementation is
#a hyper_focus heap to collapse semantic domain + reorder level-wise (not parent-descendant wise)
#a string operation on every level
#a constructive interference x + f(x) to reduce the semantic space size to match the domain collapse rate
#an interval tree on every level of the heap to dynamically manage the "potentially relevant context" - or to limit the submit size of lossy compression of "fired together grids" - we kinda move up one level until we hit the submit size
#a temporal semantic graph to bring + group those "potentially relevant context" grids - via centrality + community detection algorithms - with the hope that "potentially relevant context" grids appear in "potentially temporally relevant interval" would mean contextually relevant (we want further work on this by actually link those to the end results of feedback systems - crit logits)
#a semantic converter to convert the graph -> a domain
#a domain projection function - from the lower layer grids (heap nodes) -> upper layer grids (heap nodes)
#hyperfocus is important - even in our current transformers arch because it emphasizes the euclidean relevancy before doing continuity-related operations 

#another important concept we kinda skim through is the continuity operation of euclidean relevant context  
#we only discussed static string multiplication operations - not dynamic string ^ x operation - we need to unlock this mystery of heuristics without exploding the gradients

#we are trying to brainstorm to get the doables before starting a project
#are we graph-tensor-major or euclid-pointer-tensor major?
#thing is hyperfocus is not implementable in terms of euclid-pointer-tensor major
#without hyperfocus - we are drawing lines between irrelevant points - which is our current ML problems - how can we continuously best fit irrelevant context points? it's literally against the definition of continuity - even if you are using continuity to bring relevant context points together - it's not the definition of continuity (unless those relevant context points reordering has a continuous rule - which is absurb in most cases - we are getting into the recursive resolution here)
#so for now, there is no good way than to just get the best of both worlds - we aren't in the either territory - because graph tensor is very expensive (storage + compute) - think of 16 bytes overhead/ bit of context - euclid pointer tensor is very cheap - we are getting exactly the storage that the hardware provides with no overhead 

#or we can preprocess the semantic space to look round (human's language kinda achieve this partially) - alright - the efforts put in to make this happens actually exceed the efforts to make the mentioned thing happen
#we are, again, back to the gymnastic problem of training - we need to have coherence of training or hyperfocus
#let's talk about the example of interval tree, we want the manage the arbitrary space of x x y x z
#without loss of generality
#there is no good answer than to do an alternated version of interval tree 1/2 x x, 1/2 y y, 1/2z z
#total branching = 2 * 2 * 2 = 8 outdegrees
#the base linkage distances must reflect the euclidean distances
#1/2x x, 1/2y y
#1/2x 1/2y
#1/2x y
#x 1/2y
#x y
#the uncertainty does not sound 

#how do we do this again? by origin spreading method (like an explosion) - bfs of space grids
#we must start from the origin - assume that our space is round - and we are on radian coordinate
#and we do bread first search to extend the circumference - and we build our divide-and-conquer semantic tree from there
#this way of doing thing actually creates another semantic space - where we can do "lossy compression" of unordered_set of contextual points - and we can establish base linkages between all contextual points in the semantic coordinate

#without loss of generality
#imagine we are doing sphere compression (or multidimensional space compression)
#we circumscribe the Earth from 4/3 * pi * (4096km)^3 -> 4/3*pi m^3 - the newly created Earth is still round - but the context got "lossy compressed" on the edges (radian pies)

#says that we observed 1024 nodes fired in the last second
#but we can only submit 10 nodes to establish linkage to avoid OOM + compute fling
#we traverse up the semantic tree (to collapse semantic) + unordered_set compression - until we convert 1024 fired nodes -> most correct 10 nodes
#we have 10 fired nodes on now slowly have direct linkage to each other - via the usage of a master node - we want to reuse these master nodes - because we are on finite resources (we need to have "decay rate" of master nodes + orphan of master nodes + etc)
#so it's a temporal semantic graph space - without actual contexts - because the sole responsibiltiy of these is temporal linkage
#the temporal semantic space linkage is established base on the intense of the feedback system - and these guys do not only do inter-links but also outer-links or domain-skips 

#it's not hard to build these guys
#you must focus on the string approximation 
#you pass the first test when you can approx murmur hash
#you pass the second test when you can approx three body projections
#you pass the third test when you build a temporal system and increase the groupness of semantically equivalent contextual points (we call this centrality score - assume we link x -> y and x -> x, we want to maximize y centrality score) by a factor of 15x - 30x
#you pass the fourth test when you can run this fast - and distributed

#alright maybe we didnt split the responsibilties correctly
#let's look back to hyperfocus - hyperfocus is to bring domains' grids together + dimensional reduct to create fuzzy context representation of "how are you" or "how are you doing"
#bringing domains grids together is one responsibility - "how are you" and "how are you doing" are two different coordinates in the original domain that we want to bring together - alright - we are not being single-responsibiltiized here
#dimensional reduction is another responsibility - bit_size("how are you") + bit_size("how are you doing"), reducted to 1 bit of representation in our new domain - we are clear here
#there is no guarantee that the reducing semantic space is euclideanly related - thus the "graph temporal linkage" of node - node might not reflect the domain - range continuity like we wanted it to - "how are you" and "how are you doing" might mean two completely two different things
#for now, let's say that hyperfocus is only to reduce the semantic space by focusing on the things appear together and create another domain based on the semantic graph
#
#how about the (input - output) - we are for sure that the output is the semantic in the coordinate that we are looking for - and the input that share the same output must bend together - or euclideanly related
#so to avoid the worst case scenerio of the base domain being not euclideanly related at all - we must incorperate the "output-input topological sorting" - this must be another responsibility of hyperfocus - point is we want to have that "chain-reaction" of things completely not euclideanly related to slowly euclideanly related (not because of the domain collapses - we are talking things relatively) - and we use graph-based approach to boost that functionality
#we'll build the hyperfocus in two weeks - stay tuned

from typing import Callable

class Particle:

     def __init__(self, mass: float, position: list[float]):        
        self.mass = mass
        self.position = position 

class ForceVector:

    def __init__(self, vec: list[float]):
        self.vec = vec

class String:

    def __init__(self, string_particles: list[Particle]):
        self.string_particles = string_particles

def get_multidimensional_string(dim: int, resolution: int) -> String:
    pass

class StringOperationTree:    

    def __init__(self, left_string_tree = None, right_string_tree = None, operation_kind = None):
        self.left_string_tree   = left_string_tree
        self.right_string_tree  = right_string_tree
        self.operation_kind     = operation_kind

class OpsString:

    def __init__(self):
        pass 

OPERATION_KIND_SELF = 0
OPERATION_KIND_MUL  = 1
OPERATION_KIND_ADD  = 2 

#alright what is an operation tree? an operation tree computes the string(x) -> y
#operation tree has string * string (which is massful influence) or string + other string
#string * string is done by combinatorial pairwise operation of lhs on rhs - such is rhs moves towards lhs by the eqn f(pos, r)
#string + string is coordinate calibration - we calibrate the string by doing + operation - alright - assume we discretize an arbitrary string est surface into one dimensional pointer, then for every iteration point the new_point = sum(distance(O, i) for all i c iteration_grid)
#assume we are on radian coordinate - we have the initial space of a sphere (which is shape)

def clone_tree(root: StringOperationTree) -> StringOperationTree:

    if root == None:
        return None

    return StringOperationTree(clone_tree(root.left_string_tree), clone_tree(root.right_string_tree), root.operation_kind)

def make_operation_tree(dim: int, semantic_layer_height: int, tree_skewness: float, total_node_sz: int) -> list[StringOperationTree]:    

    if total_node_sz == 0:
        return None 

    if total_node_sz == 1:
        return StringOperationTree(None, None, OPERATION_KIND_SELF)  

    left_node_sz: int                           = min(max(1, int(total_node_sz * tree_skewness)), total_node_sz)
    right_node_sz: int                          = total_node_sz - left_node_sz
    left_tree_list: list[StringOperationTree]   = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, left_node_sz)
    right_tree_list: list[StringOperationTree]  = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, right_node_sz)
    rs_tree: list[StringOperationTree]          = []

    for i in range(len(left_tree_list)):
        for j in range(len(right_tree_list)):
            new_tree_1  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_MUL)
            new_tree_2  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_ADD)
            rs_tree     += [new_tree_1]
            rs_tree     += [new_tree_2]

    return rs_tree

#these are the three string operations that I think sufficient for every semantic approximation
#calibration, massful curvation + projection

def add_string(lhs_string: String, rhs_string: String) -> String:

    new_particle_pairs  = list(zip(lhs_string.string_particles, rhs_string.string_particles))
    new_particle        = [sum(pair) for pair in new_particle_pairs]

    return new_particle

def mul_string(lhs_string: String, rhs_string: String, eqn: Callable[[Particle, Particle], ForceVector]) -> String:

    new_string: String = String()

    for j in range(len(rhs_string.string_particles)):
        force_vec: list[ForceVector] = [] 

        for i in range(len(lhs_string.string_particles)):
            force_vec += [eqn(lhs_string.string_particles[i], rhs_string.string_particles[j])]

        new_particle = move_particle(rhs_string.string_particles[j], combine_vec(force_vec))
        add_string_particle(new_string, new_particle)

    return new_string

def project_string(string_projector: Callable[[String], String], s: String) -> String:
    pass 

def main():
    #I'll be back to write this tmr
    pass

