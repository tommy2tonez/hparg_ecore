
#alright - this is string approx - string approx problems can be defined as - stretch horizontally, stretch vertically, periodically (this is 1 - many linear (gemm)), and calibration

#let's build a tree of string operations
#string operation is essentially massful object interference on the surface - to pull the surface towards the massful object 
#let's assume our base is f(x) = 0, we have 1000 points on the surface - each massful object is going to pull the points above or below the surface (towards the massful object's coordinate)
#we want to recursively build our continuous function by this formula

#f(x) = f_previous(x) + string_interference(f_base, massful_objects)
#p(x) = string_interference(f(x), massful objects) 

#let's get to the root of intelligent semantic space - intelligent semantic space is calibration + locality compression (continuity) + periodic (periodic by dimensional reduction)
#we aren't doing random semantic space
#alright - you have a continuous function f(x) = x^2 (string) - it does not mean that it has square relation on the training data f(x) and x - but it is in some coordinate
#so it's important to do calibration (which is p(x) function) of coordinate as we are doing training - this is an oversimplification
#assume we have 20 base strings that make up the projection space 
#the first string will get calibrated 19 times (via the usage of massful objects)
#the second string will get calibrated 18 times
#the base string is our arbitrary "calibrated coordinate"

#the art of compression is machine learning - yet I think we are not there - the calibration part and the periodic - we got the base string correctly
#we need to work on the theories
#every machine learning problem is to approx the f(x) -> y projection string - by using continuous compression
#whether you are linear absolutists or string absolutists - we want to dent the continuity - and recursion building of context space (how we get the recursion building correctly is described below via the trinity tests) - and repetitive rules of continuity  

#gravity is probably on the 21th string
#electromagnetic is probably on the 20th string
#pay close attention that electromagnetic is affected by gravity but not otherwise (this is a hint that the massful_objects for electromagnetics are probably of lower string hierarchical tree)
#we call string a multi-dimensional projection shape
#this is a huge advancement in string theory - because it tells us that things, actually, can be created from multidimensional hierarchical strings (which is what transformers had been able to prove)

#think of the massful objects as semantic rules
#the first layer of particles (base_string) has to adhere to 20 semantic rules
#the second layer of parciles (base_string) has to adhere to 19 semantic rules
#etc. 
#this creates training friction for the 1st layer 2nd layer - such that the training must be able to find the semantic rules for the 1st and 2nd and 3rd layers in order to proceed
#note this very important rule of machine learning training - we need to create training friction

#on 2D euclidean coordnate - we have particle projection - and we have wave-like projection which is a 2 dimensional string
#thing is we want to use God's favorite language of "massful" for continuity compression
#what Einstein described was 4 dimensional string - and massful object would bend continuity of the 4 dimensional projection string
#Einstein was not wrong - just not sufficient - in the sense of information flux
#information flux is the maximum information that an enclosed space can receive from outer space (which is kinda maximum pingpong/s) - alright - we are talking in terms of 4 dimensionalists - how should we describe this is terms of 20 dimensionalists? it's resolution - saturated logit problems
#string quantum is right - but we can't observe another world to tell our world - the photon that went through the slit and the observed photon aren't on the same coordinate - they are on 20 dimensional projection string - and they went different direction  

#so what's our regex of building the universe? or compress continuity? Or at least this is the God's language
#f(x, n) = string_interference(f(x, n - 1), massful_objects) + f(x1, n1)
#f(x, 0) = base_string

#alright - let's twist things - God wants to confuse us - assume that we add one dimension (which is massful iterator) for massful objects - and increase our dimensiona from n -> n + 1
#what happens?
#each of them massful objects is on a different timeline
#pay very close attention to how we could simulate the universe (in our point of views and how we can differentiate the x and f(x) - hint they are all (massful objects or logits) started from the center - the Big Bang)
#all right - we are missing calibration projection - if we are string absolutists - we probably dont need calibration projections for now (because it messes up numerical stability)

#if we are absolute theorists - we must include string calibration projection
#assume the famous three-body problem - we want to be able to project the position of the three body on the 3d vs time coordinate by using stationary massful objects (on an arbitrary coordinate)

#we first have initial velocities of the three bodies
#we have the masses of the three bodies
#we have the positions of the three bodies
#we have the t0 of the three bodies 

#we first have static massful objects to rid of the mass + initial position continuity influence - and we are left with initial velocities, position and t0
#we then have static massful objects to rid of the initial velocities influence - etc.
#we then have ... to rid of the time influence

#without loss of generality, we "collapse" the semantic space to be able to approx things easier, better, faster
#the first layer already got 80% of the shape correctly
#the second layer got 10% of the shape correctly
#the third layer got 5% of the shape correctly 
#we want the hierarchical build up to allow training "friction" of strings - to simulate the actual semantic collapse and recursive build up of semantic space

#i've been thinking about string projection  
#because it is important to reorder the input domain to do continuity of different groups - without this, we are logic-impaired (in the sense that we always assume the domain to be of continuity)

#let's do things simple - calibration - massful influence - and trees 
#we want to be able to approx 3 body problem, murmur hash output space and sin cos interference as proof of concept (this is the baseline of every semantic approximator system - periodic, recursive buildup and constructive interference - if we can't get this right then I'm afraid we aren't going anywhere)
#we'll get things done today and tomorrow - then we think of how to scale this thing

#alright - we've been talking about string and projection - let's simulate string in projection environment
#assume that we are not flat-earthers
#we wanna ditch linear for massful operation

#we want to assume that we are the one element in the invisible string
#assume that we want to simulate two dimensional string
#we first want to have a linear operation - f(x) = <x, x>
#we then want to have a massful operation mass = <x1, y1>
#a pull operation should transform <x, x> -> <x + (distance * heaviness) * cos, y + (distance * heaviness) * sin>, distance = sqrt((f(x) - mass_coor) . (f(x) - mass_coor))
#alright - let's do the reverse of Chad soy boy
#assume you have the continuity of the universe, we want to unhinge the universe layers by layers
#assume that the universe is made of invisible strings and there are massful operations to bend continuity 
#what we want to do is exactly like transformer
#x = x + peel_layer(x)
#get_context_layer is essentially the, without loss of generality, (distance * heaviness) * cos, (distance * heaviness) * sin, etc 
#universe is actually like an onion - we want to peel layer by layer - by using tons of massful operations for each layers
#thing about linear is precisely that - imagine that we are on 3 dimensional string projection - we are drawing plane to peel onions - which is (1): drawing lines between irrelevant pts, (2): cause bad projection ripple effect which has bad numerical stability, (3): only good for parrot affect - sounds similar but not the actual context 

#alright - let's talk about how to build a model
#thing in machine learning is there are only two layers that we should talk about - the input layer and the output layer (because machine learning is all about numerical stability - and not about what people say)
#we want to describe the output layer based on the input layer - and how the coefficients move the output projection
#we dont really care about the middle layers and what it really does
#if we only have linear operator and we rotate the intermediate layer correctly 
#we expect y = a * x1 + b * x2 + c * x3 + ..., for y is a random output cell

#if we have self operation - we expect to see polynomial
#if we have attn - we expect to see exponent - which touches the points that the linear could not touch
#alright - how about we describe things in terms of string?

#how about we have embedding f(x) = 0 for every layer? - for x is the initial input (before embedding projection)
#and we apply the massful operation
#so we should expect to see things like f(x, n) = f(x, n - 1) * massful_objects + f0(x) * whatever
#what is wrong with this?
#that there is actually nothing wrong with this - we don't have self operation to increase polynomial order or exponent to increase the touchness of the graph
#we encode all the information in the massful_objects
#we assume there are invisible strings - and we build strings on top of each other to simulate context collapse - and we want to work on numerical stability of this  

#alright - we always want to start from - what we expect the output layer to be from the input layer and we build things from there
#I say I want the output cell to be in <x1, x2, ..., xn> space and we are able to bend the string space f(x) -> y projection - layer by layer like the formula - ((embedding(x) * massful_objects) + embedding(x1)) * massful_objects   
#alright - what are the other things that we see - we see recursive building of things (alright - attention does allow recursive building of things - because without attention the layers aren't making senses) - which we want to prove that works (or we want to twist things to allow this to work)
#what other things that we see, that embedding(x) and embedding(x1) probably look the same - and we want to reuse the massful_objects rule for all other cells

#we gonna gucci later babe - just have faith for probably 2 years - I'll try to code the regex + path_optimization + randomization + convergence analysis + derivative of gradients + etc.
#dont trust Chad - all he does is offensive meme and closing things

#what if I tell you that AI is three body of three body of three body of ... problem
#let's investigate the three body problem https://en.wikipedia.org/wiki/Three-body_problem
#it's https://en.wikipedia.org/wiki/Puiseux_series, proved by Finnish mathematician Karl Fritiof Sundman - not Linus Torvards
#alright - we have f(x) = puiseux_serieries
#we probably want to do f1(f2(f3(x))) - this is where we are at - the transformers and their exponents

#how about law of momentum conservation?
#alright - so what's momentum? momentum is the moral of the story behind the forever spins around each other of the-three body problem
#God - the old man - does not have infinite resource - he needs to store the moral of the story in order to bend continuity
#in other words, he needs to have story coherence in order to do story telling of what's next in the continuity story
#back to the gymnastic story - a girl spins in a complex motion - but there's no moral of the story for continuity  
#how do we actually incorporate this in mathematics? - could this "moral of the story" be described in mathematics terms? 
#yes - but it would break numerical stability of the result - I'll be back to solve the problem today - 

#finding the momentums cannot be done via differential methods - we break numerical stabilities when the uncertainty exceeds the gap closing of gradient approximation
#linear, eigen values and eigen vectors are cool - but what happen if we do string multiplication?
#back to the gravity problem - f(t) = string ^ t
#there aren't many ways to do this correctly (at least not the differential roads) - but the f(t) has a unique footprint that narrows the function down to only a few operations to approximate (we are approxing string) - alright - we are back to the decoding problem and semantic space + information storage theorem
#maybe differential and Newton are only good for approximating classical local optimal solutions
#alright - this works only if we do precalculation to do heuristic pruning of what's possible version of string - we want to discretize + precompute this - because this is a very important concept of neural network - the dynamic continuity
#
#so we have changed our coordinate to radians - and we have all the initial string right in the middle of the coordinate (the origin) which has a sphere shape - and we have a sphere shape of massful objects in the outer layer, we are doing an enclosed shape of continuity (this is an important concept) 
#thing is this way of doing things have a very smooth curve of gradient - and we are not stuck in the gradient valley like polynomial methods 
#and this actually simulates the actual context collapse efficiently - the recursive build up - this is up for debate - it only is if the projection string is kinda round - not like zig-zag shape which affects the uniform distribution of massful objects which require a string projection in order to rebalance the uniform responsibility - or skewed responsibility  
#we want to mess with numerical stabilities in the newton approx, and we dont want to stuck in the valley - so it's the way of doing things

from typing import Callable

class Particle:

     def __init__(self, mass: float, position: list[float]):        
        self.mass = mass
        self.position = position 

class ForceVector:

    def __init__(self, vec: list[float]):
        self.vec = vec

class String:

    def __init__(self, string_particles: list[Particle]):
        self.string_particles = string_particles

def get_multidimensional_string(dim: int, resolution: int) -> String:
    pass

class StringOperationTree:    

    def __init__(self, left_string_tree = None, right_string_tree = None, operation_kind = None):
        self.left_string_tree   = left_string_tree
        self.right_string_tree  = right_string_tree
        self.operation_kind     = operation_kind

class OpsString:

    def __init__(self):
        pass 

OPERATION_KIND_SELF = 0
OPERATION_KIND_MUL  = 1
OPERATION_KIND_ADD  = 2 

#alright what is an operation tree? an operation tree computes the string(x) -> y
#operation tree has string * string (which is massful influence) or string + other string
#string * string is done by combinatorial pairwise operation of lhs on rhs - such is rhs moves towards lhs by the eqn f(pos, r)
#string + string is coordinate calibration - we calibrate the string by doing + operation - alright - assume we discretize an arbitrary string est surface into one dimensional pointer, then for every iteration point the new_point = sum(distance(O, i) for all i c iteration_grid)
#assume we are on radian coordinate - we have the initial space of a sphere (which is shape)

def clone_tree(root: StringOperationTree) -> StringOperationTree:

    if root == None:
        return None

    return StringOperationTree(clone_tree(root.left_string_tree), clone_tree(root.right_string_tree), root.operation_kind)

def make_operation_tree(dim: int, semantic_layer_height: int, tree_skewness: float, total_node_sz: int) -> list[StringOperationTree]:    

    if total_node_sz == 0:
        return None 

    if total_node_sz == 1:
        return StringOperationTree(None, None, OPERATION_KIND_SELF)  

    left_node_sz: int                           = min(max(1, int(total_node_sz * tree_skewness)), total_node_sz)
    right_node_sz: int                          = total_node_sz - left_node_sz
    left_tree_list: list[StringOperationTree]   = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, left_node_sz)
    right_tree_list: list[StringOperationTree]  = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, right_node_sz)
    rs_tree: list[StringOperationTree]          = []

    for i in range(len(left_tree_list)):
        for j in range(len(right_tree_list)):
            new_tree_1  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_MUL)
            new_tree_2  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_ADD)
            rs_tree     += [new_tree_1]
            rs_tree     += [new_tree_2]

    return rs_tree

#these are the three string operations that I think sufficient for every semantic approximation
#calibration, massful curvation + projection

def add_string(lhs_string: String, rhs_string: String) -> String:

    new_particle_pairs  = list(zip(lhs_string.string_particles, rhs_string.string_particles))
    new_particle        = [sum(pair) for pair in new_particle_pairs]

    return new_particle

def mul_string(lhs_string: String, rhs_string: String, eqn: Callable[[Particle, Particle], ForceVector]) -> String:

    new_string: String = String()

    for j in range(len(rhs_string.string_particles)):
        force_vec: list[ForceVector] = [] 

        for i in range(len(lhs_string.string_particles)):
            force_vec += [eqn(lhs_string.string_particles[i], rhs_string.string_particles[j])]

        new_particle = move_particle(rhs_string.string_particles[j], combine_vec(force_vec))
        add_string_particle(new_string, new_particle)

    return new_string

def project_string(string_projector: Callable[[String], String], s: String) -> String:
    pass 

def main():
    #I'll be back to write this tmr
    pass

