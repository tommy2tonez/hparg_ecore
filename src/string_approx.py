
#alright - this is string approx - string approx problems can be defined as - stretch horizontally, stretch vertically, periodically (this is 1 - many linear (gemm)), and calibration

#let's build a tree of string operations
#string operation is essentially massful object interference on the surface - to pull the surface towards the massful object 
#let's assume our base is f(x) = 0, we have 1000 points on the surface - each massful object is going to pull the points above or below the surface (towards the massful object's coordinate)
#we want to recursively build our continuous function by this formula

#f(x) = f_previous(x) + string_interference(f_base, massful_objects)
#p(x) = string_interference(f(x), massful objects) 

#let's get to the root of intelligent semantic space - intelligent semantic space is calibration + locality compression (continuity) + periodic (periodic by dimensional reduction)
#we aren't doing random semantic space
#alright - you have a continuous function f(x) = x^2 (string) - it does not mean that it has square relation on the training data f(x) and x - but it is in some coordinate
#so it's important to do calibration (which is p(x) function) of coordinate as we are doing training - this is an oversimplification
#assume we have 20 base strings that make up the projection space 
#the first string will get calibrated 19 times (via the usage of massful objects)
#the second string will get calibrated 18 times
#the base string is our arbitrary "calibrated coordinate"

#the art of compression is machine learning - yet I think we are not there - the calibration part and the periodic - we got the base string correctly
#we need to work on the theories
#every machine learning problem is to approx the f(x) -> y projection string - by using continuous compression
#whether you are linear absolutists or string absolutists - we want to dent the continuity - and recursion building of context space (how we get the recursion building correctly is described below via the trinity tests) - and repetitive rules of continuity  

#gravity is probably on the 21th string
#electromagnetic is probably on the 20th string
#pay close attention that electromagnetic is affected by gravity but not otherwise (this is a hint that the massful_objects for electromagnetics are probably of lower string hierarchical tree)
#we call string a multi-dimensional projection shape
#this is a huge advancement in string theory - because it tells us that things, actually, can be created from multidimensional hierarchical strings (which is what transformers had been able to prove)

#think of the massful objects as semantic rules
#the first layer of particles (base_string) has to adhere to 20 semantic rules
#the second layer of parciles (base_string) has to adhere to 19 semantic rules
#etc. 
#this creates training friction for the 1st layer 2nd layer - such that the training must be able to find the semantic rules for the 1st and 2nd and 3rd layers in order to proceed
#note this very important rule of machine learning training - we need to create training friction

#on 2D euclidean coordnate - we have particle projection - and we have wave-like projection which is a 2 dimensional string
#thing is we want to use God's favorite language of "massful" for continuity compression
#what Einstein described was 4 dimensional string - and massful object would bend continuity of the 4 dimensional projection string
#Einstein was not wrong - just not sufficient - in the sense of information flux
#information flux is the maximum information that an enclosed space can receive from outer space (which is kinda maximum pingpong/s) - alright - we are talking in terms of 4 dimensionalists - how should we describe this is terms of 20 dimensionalists? it's resolution - saturated logit problems
#string quantum is right - but we can't observe another world to tell our world - the photon that went through the slit and the observed photon aren't on the same coordinate - they are on 20 dimensional projection string - and they went different direction  

#so what's our regex of building the universe? or compress continuity? Or at least this is the God's language
#f(x, n) = string_interference(f(x, n - 1), massful_objects) + f(x1, n1)
#f(x, 0) = base_string

#alright - let's twist things - God wants to confuse us - assume that we add one dimension (which is massful iterator) for massful objects - and increase our dimensiona from n -> n + 1
#what happens?
#each of them massful objects is on a different timeline
#pay very close attention to how we could simulate the universe (in our point of views and how we can differentiate the x and f(x) - hint they are all (massful objects or logits) started from the center - the Big Bang)
#all right - we are missing calibration projection - if we are string absolutists - we probably dont need calibration projections for now (because it messes up numerical stability)

#if we are absolute theorists - we must include string calibration projection
#assume the famous three-body problem - we want to be able to project the position of the three body on the 3d vs time coordinate by using stationary massful objects (on an arbitrary coordinate)

#we first have initial velocities of the three bodies
#we have the masses of the three bodies
#we have the positions of the three bodies
#we have the t0 of the three bodies 

#we first have static massful objects to rid of the mass + initial position continuity influence - and we are left with initial velocities, position and t0
#we then have static massful objects to rid of the initial velocities influence - etc.
#we then have ... to rid of the time influence

#without loss of generality, we "collapse" the semantic space to be able to approx things easier, better, faster
#the first layer already got 80% of the shape correctly
#the second layer got 10% of the shape correctly
#the third layer got 5% of the shape correctly 
#we want the hierarchical build up to allow training "friction" of strings - to simulate the actual semantic collapse and recursive build up of semantic space

#i've been thinking about string projection  
#because it is important to reorder the input domain to do continuity of different groups - without this, we are logic-impaired (in the sense that we always assume the domain to be of continuity)

#let's do things simple - calibration - massful influence - and trees 
#we want to be able to approx 3 body problem, murmur hash output space and sin cos interference as proof of concept (this is the baseline of every semantic approximator system - periodic, recursive buildup and constructive interference - if we can't get this right then I'm afraid we aren't going anywhere)
#we'll get things done today and tomorrow - then we think of how to scale this thing

#alright - we've been talking about string and projection - let's simulate string in projection environment
#assume that we are not flat-earthers
#we wanna ditch linear for massful operation

#we want to assume that we are the one element in the invisible string
#assume that we want to simulate two dimensional string
#we first want to have a linear operation - f(x) = <x, x>
#we then want to have a massful operation mass = <x1, y1>
#a pull operation should transform <x, x> -> <x + (distance * heaviness) * cos, y + (distance * heaviness) * sin>, distance = sqrt((f(x) - mass_coor) . (f(x) - mass_coor))
#alright - let's do the reverse of Chad soy boy
#assume you have the continuity of the universe, we want to unhinge the universe layers by layers
#assume that the universe is made of invisible strings and there are massful operations to bend continuity 
#what we want to do is exactly like transformer
#x = x + peel_layer(x)
#get_context_layer is essentially the, without loss of generality, (distance * heaviness) * cos, (distance * heaviness) * sin, etc 
#universe is actually like an onion - we want to peel layer by layer - by using tons of massful operations for each layers
#thing about linear is precisely that - imagine that we are on 3 dimensional string projection - we are drawing plane to peel onions - which is (1): drawing lines between irrelevant pts, (2): cause bad projection ripple effect which has bad numerical stability, (3): only good for parrot affect - sounds similar but not the actual context 

#alright - let's talk about how to build a model
#thing in machine learning is there are only two layers that we should talk about - the input layer and the output layer (because machine learning is all about numerical stability - and not about what people say)
#we want to describe the output layer based on the input layer - and how the coefficients move the output projection
#we dont really care about the middle layers and what it really does
#if we only have linear operator and we rotate the intermediate layer correctly 
#we expect y = a * x1 + b * x2 + c * x3 + ..., for y is a random output cell

#if we have self operation - we expect to see polynomial
#if we have attn - we expect to see exponent - which touches the points that the linear could not touch
#alright - how about we describe things in terms of string?

#how about we have embedding f(x) = 0 for every layer? - for x is the initial input (before embedding projection)
#and we apply the massful operation
#so we should expect to see things like f(x, n) = f(x, n - 1) * massful_objects + f0(x) * whatever
#what is wrong with this?
#that there is actually nothing wrong with this - we don't have self operation to increase polynomial order or exponent to increase the touchness of the graph
#we encode all the information in the massful_objects
#we assume there are invisible strings - and we build strings on top of each other to simulate context collapse - and we want to work on numerical stability of this  

#alright - we always want to start from - what we expect the output layer to be from the input layer and we build things from there
#I say I want the output cell to be in <x1, x2, ..., xn> space and we are able to bend the string space f(x) -> y projection - layer by layer like the formula - ((embedding(x) * massful_objects) + embedding(x1)) * massful_objects   
#alright - what are the other things that we see - we see recursive building of things (alright - attention does allow recursive building of things - because without attention the layers aren't making senses) - which we want to prove that works (or we want to twist things to allow this to work)
#what other things that we see, that embedding(x) and embedding(x1) probably look the same - and we want to reuse the massful_objects rule for all other cells

#we gonna gucci later babe - just have faith for probably 2 years - I'll try to code the regex + path_optimization + randomization + convergence analysis + derivative of gradients + etc.
#dont trust Chad - all he does is offensive meme and closing things

#what if I tell you that AI is three body of three body of three body of ... problem
#let's investigate the three body problem https://en.wikipedia.org/wiki/Three-body_problem
#it's https://en.wikipedia.org/wiki/Puiseux_series, proved by Finnish mathematician Karl Fritiof Sundman - not Linus Torvards
#alright - we have f(x) = puiseux_serieries
#we probably want to do f1(f2(f3(x))) - this is where we are at - the transformers and their exponents

#how about law of momentum conservation?
#alright - so what's momentum? momentum is the moral of the story behind the forever spins around each other of the-three body problem
#God - the old man - does not have infinite resource - he needs to store the moral of the story in order to bend continuity
#in other words, he needs to have story coherence in order to do story telling of what's next in the continuity story
#back to the gymnastic story - a girl spins in a complex motion - but there's no moral of the story for continuity  
#how do we actually incorporate this in mathematics? - could this "moral of the story" be described in mathematics terms? 
#yes - but it would break numerical stability of the result - I'll be back to solve the problem today - 

#finding the momentums cannot be done via differential methods - we break numerical stabilities when the uncertainty exceeds the gap closing of gradient approximation
#linear, eigen values and eigen vectors are cool - but what happen if we do string multiplication?
#back to the gravity problem - f(t) = string ^ t
#there aren't many ways to do this correctly (at least not the differential roads) - but the f(t) has a unique footprint that narrows the function down to only a few operations to approximate (we are approxing string) - alright - we are back to the decoding problem and semantic space + information storage theorem
#maybe differential and Newton are only good for approximating classical local optimal solutions
#alright - this works only if we do precalculation to do heuristic pruning of what's possible version of string - we want to discretize + precompute this - because this is a very important concept of neural network - the dynamic continuity
#
#so we have changed our coordinate to radians - and we have all the initial string right in the middle of the coordinate (the origin) which has a sphere shape - and we have a sphere shape of massful objects in the outer layer, we are doing an enclosed shape of continuity (this is an important concept) 
#thing is this way of doing things have a very smooth curve of gradient - and we are not stuck in the gradient valley like polynomial methods 
#and this actually simulates the actual context collapse efficiently - the recursive build up - this is up for debate - it only is if the projection string is kinda round - not like zig-zag shape which affects the uniform distribution of massful objects which require a string projection in order to rebalance the uniform responsibility - or skewed responsibility  
#we want to mess with numerical stabilities in the newton approx, and we dont want to stuck in the valley - so it's the way of doing things
#alright - let's talk about the numerical stability - we have two spheres
#Dad talked about the gradient valley of curves - such is there is a non-returnable point of shape - such is the "hardness" of shifting from the shape to another possible shape from the original point is impossible
#we must circumscribe the closing circumference gap (middle theorem) to limit the non-returnables, and we kinda stay in the acceptable fluid zone of gradient moving 
#so the deeper the valley - the harder it is to shape the curve - or the distribution of possible shapes is skewed in the direction of the valley
#so we must do string projection - to "normalize" the input string - and keep it in "acceptable" gradient zone

#in a general perspective, we have f(x) -> y is projection string
#we have possible shapes - and we have the possibility of moving from one shape to another (we kinda want to flex this area of possibility to not stuck in the gradient valley)
#we have string altitude normalization - we normalize a string to do shape bending better - as described above
#we have string ^ t heuristic approximator - we need special method (not differential) to converge this approxmiation
#we need to make sure that the uncertainty of gradients and destructive interference do not exceed the learning
#things gonna be hard but I think its possible
#this mimics how our brain actually works
#whenever a logit moves - we move towards the result - but losse the old information - the question is given the appearance of the results in a certain window - what's the window of uncertainty of the not appeared results? How fast do we actually forget things to burn new information?
#a fully burnt brain does not move logits as fast as a newborn brain - in the way that a logit move does not touch all the projection string by r^2 or r^3  
#and the lower layers are actually very important to predict | train the upper layers - given a math semantics x^2 + y^2 + z^2 - our brains collapse the math semantic space and do string projection to a new space where we could do another semantic collapse without affecting the old space - things get confusing here
#if we mess up the lower layers - we steer away from the correct path - and the incorrectness might diverge - not converge
#in order to make the semantic collapse "real" - we must do string normalization - assume our example of math semantic collapse as murmur hash
#in murmur - we shift, multiply, rotate, etc - when we shift - we move it to another semantic space - we multiply in the new semantic space and we rotate in another new semantic space
#good string normalization and numerical stabilities are hard to do

#we probably can mimic the string projections by calibrations - this is hard - without actual string projection we are bending continuity of irrelevant data
#this is hence the f(x) = x + peel(x)
#how do we actually describe this in mathematical terms?
#imagine that we have x y z on the slot 1, 10, 20
#we want to reorder the x y z to 0, 1, 2 - and bend the continuity there - because they are relevant terms in mathematics - and we want to map it back to 1, 10, 20 and do a constructive interference operation - that's kinda the idea of context space collapse of euclidean irrelevant data
#this is a position reordering operations
#position reordering operations (or calibration) are actually the tough part - do we do mapping based on set frequencies? or continuous functions? things that appear together should be grouped together - this is part of the temporal linkage part of our brains - things fired together linked together  
#a simple linear(x) is too vague to describe this functionality
#how about a discretized tree of semantic space - things fired together are interval tree responsibility

#alright, all the difficulty is because of we want to group relevant things together - things fired together - we map it to another semantic space (and now we have another things-fired-together) - we do continuity bending there - because they are relevant - we map it back to our semantic space - we do constructive interference
#and we group those fired together things again - we keep doing that thing, which is called semantic space collapse of irrelevant data - look back to our example of murmurhash - this is not a necessity if the semantic space is nice and round

#our state of the art is actually not that we do context collapse (which is briefly described via the solution to murmur hash approx) but the reordering of context space - to bend euclidean irrelevant things together
#assume this simple model of doing so
#given that our input layer is the base
#given that we discretize the space into grids
#given our next to base layer is the layer of the things fired together in the base
#given our next to the next to base layer is the layer of things fired together in the next to base
#what do we want to do with this? for every layer not base - we can do an inorder-search of base to create a newly-reordered semantic space without breaking numerical stabilities
#the attention was to solve this specific problem (the problem of bending euclidean irrelevant things) - but in a numerical instable way
#the actual model of reordering must be of complex forms - and involve algorithms and not maths - we'll talk about this later - this is hard

#alright the topics we are going cover today are hyper focus subspace + discretization of euclidean space + space dimensional reduction + dimensional expansion
#we have talked about the things fired together grouped together by temporal linkage - all linked to a master node - which is again fired together and linked to another master node
#we then have our newly created semantic space in terms of graph - which represents temporally relevant things
#but we want to bend this this in euclidean coordinate (because we have a well establised string theory of how relevant things bend together) - so we must invent a way to convert graph semantic space -> euclidean semantic space - by, without loss of generality, the formula euclidean_distance(node1, node2) = shortest_weighted_raph_distance(node1, node2) or maxflow(node1, node2) or by whatever lossless compression that we can reconstruct the semantic graph from
#alright - things aren't perfect - we can only fuzzy approx the euclidean coordinate - because there is no perfect answer
#how about we do it again? rinse and repeat in our newly created euclidean coordinate

#discretization of euclidean space is nothing but interval (in terms of for loop) - we have intervalx(intervaly(intervalz)) - and we grid the (dx x dy x dz) 

#let's cover the space dimensional reduction | expansion in a nutshell
#let's get back to the example of x y z
#assume we are on three dimensional domain - the chances of x y z "euclideanly" collaborate are higher than on one dimensional domain (because we can move in the 2nd and 3rd dimension to relatively bend things without compromising the euclidean_distance(node1, node2)) 
#what's the difference between 3 dimensional domain and 1 dimensional domain - there is essentially no difference if the one dimensional domain is domain(x) * domain(y) * domain(z) - which has bijective relation to the 3 dimensional domain
#the only difference that we are mentioning is the euclidean distance - which is important for bending strings by our described methods
#so increasing dimensions helps continuity bending of relevant groups

#let's get back to our tokenization methods - do we enumerate the vocabularies to create an arbitrary semantic space - or we do byte stream - or we do bit stream
#bit stream is the ultimate goal of information technology - because all the algorithms mostly operate on bit levels
#alright - so we increase the dimensions of the semantic space - which is, by our definition, a good thing - because it increases the groupabilities
#so what's hyper focus - hyper focus is the abiltiy of temporal linkage of tensors - recursively to create an arbitrary semantic spaces to increase the bendability of relevant tensors  

#how precisely do we build hyper focus? by using weight decay on edges and broadcasts from input nodes and temporal group of lit up nodes (weakly connected component)
#so hyper focus is a centrality algorithm - and linkage of lit up nodes is community detection algorithm
#alright - we need to have decay rate of community and edges because we are on finite resources
#the only responsibility of hyper focus is to reorder the semantic domain (community detection) of relevant things (in terms of input space)
#and we want to construct a euclidean semantic space from the hyper focus graph space to bend continuity there
#and we want to reconstruct the hyper focus graph space from the bent euclidean space to update the hyper focus graph space which will be reflected on the original euclidean space
#or we can adhere to the formula of euclidean_distance(node1, node2) = maxflow(node1, node2) - and we have bijective domain relation to the original euclidean space - which we can directly update on
#hyper focus and string continuity operations are two sides of the same coin - can't exist without another - unless the projection string is round 
#we'll work more on the theory
#our grand plan is to reduce the complexity of constructive interference of n! space (n is the domain's range, n c N)  -> 32 reordered space by using hyper focus with the hope that hyper focus would bring relevant things together to help our continuity bending
#i thought we were frens fr 
#frens dont smash my phones

from typing import Callable

class Particle:

     def __init__(self, mass: float, position: list[float]):        
        self.mass = mass
        self.position = position 

class ForceVector:

    def __init__(self, vec: list[float]):
        self.vec = vec

class String:

    def __init__(self, string_particles: list[Particle]):
        self.string_particles = string_particles

def get_multidimensional_string(dim: int, resolution: int) -> String:
    pass

class StringOperationTree:    

    def __init__(self, left_string_tree = None, right_string_tree = None, operation_kind = None):
        self.left_string_tree   = left_string_tree
        self.right_string_tree  = right_string_tree
        self.operation_kind     = operation_kind

class OpsString:

    def __init__(self):
        pass 

OPERATION_KIND_SELF = 0
OPERATION_KIND_MUL  = 1
OPERATION_KIND_ADD  = 2 

#alright what is an operation tree? an operation tree computes the string(x) -> y
#operation tree has string * string (which is massful influence) or string + other string
#string * string is done by combinatorial pairwise operation of lhs on rhs - such is rhs moves towards lhs by the eqn f(pos, r)
#string + string is coordinate calibration - we calibrate the string by doing + operation - alright - assume we discretize an arbitrary string est surface into one dimensional pointer, then for every iteration point the new_point = sum(distance(O, i) for all i c iteration_grid)
#assume we are on radian coordinate - we have the initial space of a sphere (which is shape)

def clone_tree(root: StringOperationTree) -> StringOperationTree:

    if root == None:
        return None

    return StringOperationTree(clone_tree(root.left_string_tree), clone_tree(root.right_string_tree), root.operation_kind)

def make_operation_tree(dim: int, semantic_layer_height: int, tree_skewness: float, total_node_sz: int) -> list[StringOperationTree]:    

    if total_node_sz == 0:
        return None 

    if total_node_sz == 1:
        return StringOperationTree(None, None, OPERATION_KIND_SELF)  

    left_node_sz: int                           = min(max(1, int(total_node_sz * tree_skewness)), total_node_sz)
    right_node_sz: int                          = total_node_sz - left_node_sz
    left_tree_list: list[StringOperationTree]   = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, left_node_sz)
    right_tree_list: list[StringOperationTree]  = make_operation_tree(dim, semantic_layer_height - 1, tree_skewness, right_node_sz)
    rs_tree: list[StringOperationTree]          = []

    for i in range(len(left_tree_list)):
        for j in range(len(right_tree_list)):
            new_tree_1  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_MUL)
            new_tree_2  = StringOperationTree(clone_tree(left_tree_list[i]), clone_tree(right_tree_list[j]), OPERATION_KIND_ADD)
            rs_tree     += [new_tree_1]
            rs_tree     += [new_tree_2]

    return rs_tree

#these are the three string operations that I think sufficient for every semantic approximation
#calibration, massful curvation + projection

def add_string(lhs_string: String, rhs_string: String) -> String:

    new_particle_pairs  = list(zip(lhs_string.string_particles, rhs_string.string_particles))
    new_particle        = [sum(pair) for pair in new_particle_pairs]

    return new_particle

def mul_string(lhs_string: String, rhs_string: String, eqn: Callable[[Particle, Particle], ForceVector]) -> String:

    new_string: String = String()

    for j in range(len(rhs_string.string_particles)):
        force_vec: list[ForceVector] = [] 

        for i in range(len(lhs_string.string_particles)):
            force_vec += [eqn(lhs_string.string_particles[i], rhs_string.string_particles[j])]

        new_particle = move_particle(rhs_string.string_particles[j], combine_vec(force_vec))
        add_string_particle(new_string, new_particle)

    return new_string

def project_string(string_projector: Callable[[String], String], s: String) -> String:
    pass 

def main():
    #I'll be back to write this tmr
    pass

